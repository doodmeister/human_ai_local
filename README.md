# Human-AI Cognition Framework# Human-AI Cognition Framework



## Overview## Overview

A production-grade, biologically-inspired cognitive architecture for human-like memory, attention, reasoning, and executive control in AI systems. Features persistent, explainable memory structures with **interactive chat interface**, **dynamic memory controls**, and **real-time consolidation**.A production-grade, biologically-inspired cognitive architecture for human-like memory, attention, reasoning, and executive control in AI systems. Features persistent, explainable memory structures, modular processing, advanced neural integration, executive functioning, and comprehensive error handling.



**What makes this unique:**---

- üß† **STM ‚Üí LTM Pipeline**: Short-term memories naturally consolidate into long-term storage

- üéöÔ∏è **Dynamic Threshold Control**: Real-time slider to adjust what gets remembered## üöÄ Latest Update: Executive Functioning System (July 2025)

- üåô **Dream Cycle**: One-click memory consolidation from STM to LTM

- üìä **Full Transparency**: See salience scores, memory hits, and consolidation decisions### Production-Grade Executive Control System

- üîß **Production-Ready**: Thread-safe, error-handled, comprehensively testedThe Executive Functioning System represents the "prefrontal cortex" of the cognitive architecture, providing strategic planning, decision-making, and resource management capabilities:



---#### **Five Core Executive Components**

- **Goal Manager**: Hierarchical goal tracking with priority-based resource allocation

## üöÄ Quick Start- **Task Planner**: Goal decomposition into executable tasks with dependency management  

- **Decision Engine**: Multi-criteria decision making with confidence assessment

### **Installation**- **Cognitive Controller**: Resource allocation and cognitive state monitoring

```bash- **Executive Agent**: Central orchestrator integrating all components

# 1. Clone and navigate to the repository

cd human_ai_local#### **Key Executive Features**

- **Strategic Planning**: Long-term goal management with hierarchical parent-child relationships

# 2. Create and activate virtual environment- **Multi-Criteria Decision Making**: Weighted scoring across multiple criteria with confidence assessment

python -m venv venv- **Resource Management**: Dynamic allocation of attention, memory, processing, energy, and time

source venv/Scripts/activate  # Windows Git Bash- **Cognitive Mode Management**: FOCUSED, MULTI_TASK, EXPLORATION, REFLECTION, RECOVERY modes

# or: source venv/bin/activate  # Linux/Mac- **Performance Monitoring**: Real-time executive effectiveness tracking and optimization suggestions

- **Adaptive Behavior**: Learns and adapts based on performance feedback and outcomes

# 3. Install dependencies

pip install -r requirements.txt#### **Executive Processing Pipeline**

The executive system provides human-like cognitive control through:

# 4. Set up environment variables

cp .env.example .env1. **Input Analysis**: Intent recognition, complexity assessment, urgency detection

# Edit .env and add your OpenAI API key2. **Goal Assessment**: Create/update hierarchical goals with priority management

```3. **Task Planning**: Decompose goals into executable tasks with dependency resolution

4. **Decision Making**: Multi-criteria analysis with confidence-weighted selection

### **üöÄ One-Command Startup (Recommended)**5. **Resource Allocation**: Dynamic cognitive resource distribution and monitoring

```bash6. **Execution Monitoring**: Track progress, performance, and adapt strategies

# Option 1: Git Bash / Linux / Mac7. **Reflection**: Periodic self-assessment and strategy optimization

./start_george.sh

#### **Production-Ready Implementation**

# Option 2: Any terminal- **1,500+ Lines**: Production-grade code with comprehensive error handling

python start_george.py- **Type Safety**: Full type annotations with runtime validation

```- **Thread Safety**: Concurrent operations with proper locking mechanisms

- **Performance Optimization**: O(n log n) complexity for hierarchical operations

These scripts automatically:- **Test Coverage**: 100% pass rate with comprehensive integration testing

- ‚úÖ Detect and activate your virtual environment

- ‚úÖ Start the API server on http://localhost:8000#### **Integration with Cognitive Architecture**

- ‚úÖ Launch the Streamlit chat interface on http://localhost:8501- **Memory Integration**: Goals, tasks, and decisions stored across STM/LTM systems

- ‚úÖ Open your browser automatically- **Attention Coordination**: Dynamic attention allocation based on task priorities

- **Neural Enhancement**: DPAD network provides attention boosts for high-priority goals

### **üîß Manual Startup (Advanced)**- **Dream State Support**: Background consolidation of executive experiences

```bash- **Real-Time Adaptation**: Continuous optimization based on cognitive performance

# Terminal 1: Start the backend API

python start_server.py### **Executive System Results**

Comprehensive testing demonstrates:

# Terminal 2: Start the Streamlit interface- **Strategic Thinking**: Hierarchical goal management with parent-child relationships

python -m streamlit run scripts/george_streamlit_chat.py --server.port 8501- **Complex Decision Making**: Multi-criteria analysis with 0.58+ confidence scores

```- **Resource Optimization**: Real-time cognitive load balancing and mode transitions

- **Task Coordination**: Automated goal decomposition with dependency resolution

### **üìç Access Points**- **Performance Monitoring**: Executive efficiency tracking with adaptation recommendations

- **Chat Interface**: http://localhost:8501 (Streamlit minimal chat)

- **API Documentation**: http://localhost:8000/docs (Swagger UI)---

- **Health Check**: http://localhost:8000/health

## üìä Consolidation & Performance Metrics (August 2025)

---

### Consolidation Pipeline Visibility

## üí¨ Chat Interface FeaturesThe chat performance endpoint (`/agent/chat/performance`) now surfaces consolidation metrics alongside latency and throughput:



The minimal Streamlit interface (`scripts/george_streamlit_chat.py`) provides a clean, transparent window into the cognitive system:Returned structure (fields only):

```

### **Core Functionality**{

- **STM ‚Üí LTM ‚Üí LLM Pipeline**: Each message searches Short-Term Memory first, falls back to Long-Term Memory, then passes relevant context to the language model  "latency_p95_ms": <float>,

- **Context Visibility**: See exactly which memory systems contributed to each response (STM/LTM/recent/attention/executive)  "target_p95_ms": <float|None>,

- **Performance Metrics**: Real-time latency, memory hit counts, and fallback status  "performance_degraded": <bool>,

  "ema_turn_latency_ms": <float>,

### **Interactive Controls** (Sidebar)  "chat_turns_per_sec": <float>,

  "consolidation": {

#### **Memory Controls**    "counters": {

- **üéöÔ∏è STM Consolidation Threshold Slider** (0.0 - 1.0)      "stm_store_total": <int>,        # Total user turns stored in STM

  - **Lower values (0.25-0.35)**: Captures casual conversation      "ltm_promotions_total": <int>    # Successful promotions to LTM

  - **Default (0.55)**: Balanced - captures moderately important messages    },

  - **Higher values (0.70-0.80)**: Only captures emphatic, emotionally-charged messages    "promotion_age_p95_seconds": <float> # p95 age (s) of promoted turns (STM dwell time)

  - Real-time adjustment - no server restart needed!  }

}

- **üåô Dream Cycle Button**```

  - Triggers STM ‚Üí LTM consolidation on demand

  - Promotes memories that meet criteria:## üß≠ Metacognitive Adaptation & Self-Monitoring (August 2025)

    - Rehearsals ‚â• 2 (mentioned multiple times)

    - Age ‚â• 5 secondsRecent enhancements added adaptive, self-regulating behaviors to the chat pipeline:

    - Importance ‚â• 0.4

  - Shows detailed results in expandable section**Snapshot System**

- Periodic metacog snapshots every `metacog_turn_interval` turns (dynamic 2‚Äì10 range)

#### **Chat Options**- Snapshot fields: performance latency p95 + degraded flag, consolidation selectivity, STM utilization/capacity, promotion age p95, last consolidation status

- ‚úÖ Include memory retrieval- Stored to LTM (best-effort) with `type=meta_reflection` and maintained in an in-memory ring buffer

- ‚úÖ Include attention signals

- üîç Include trace details (debug mode)**Adaptive Controls**

- üí≠ Request reflection- Adaptive retrieval limit: temporary reduction of `max_context_items` when performance degraded or STM utilization ‚â•85%

- Adaptive consolidation thresholds: temporary salience tightening under load/degradation

### **Transparency Features**- Dynamic snapshot interval modulation: tightens under pressure, relaxes during stability



#### **Context Used** (per message)**Advisory Context Injection**

Each response shows exactly what memory items were retrieved:- Injects explicit metacog advisory items (`source_system=metacog`) when performance degraded or STM high utilization for explainability

```

1. [stm] I went to the mall yesterday**Metrics & Observability**

   reason: recent_activation_match- Counters: `metacog_snapshots_total`, `metacog_advisory_items_total`, `metacog_stm_high_util_events_total`, `metacog_performance_degraded_events_total`, `adaptive_retrieval_applied_total`, plus prospective reminder injection counters

2. [ltm] User prefers outdoor activities- Performance endpoint now returns `metacog` section with counters + current dynamic interval

   reason: semantic_similarity_0.82

3. [recent] Previous conversation turn**Configuration**

   reason: recent_context- Centralized via `ChatConfig` additions: `metacog_turn_interval`, `metacog_snapshot_history_size` (ring buffer)

```- All adaptive behaviors are non-destructive‚Äîoriginal configuration restored each turn after temporary adjustments



#### **Captured Memories** (per message)**Testing**

See what facts/preferences/goals the system extracted:- `test_chat_metacog_metrics.py`, `test_chat_adaptive_retrieval.py`, `test_chat_dynamic_metacog_interval.py` validate counters, retrieval reduction, and interval modulation

```

[fact] User visited the mall - frequency 2These features collectively provide real-time self-awareness and automatic load shedding to preserve latency and context quality.

[preference] User enjoys shopping [reinforced]

```## ‚è∞ Prospective Memory Reminders (In-Memory Beta)



#### **Debug Metrics** (per message)An initial lightweight Prospective Memory module enables scheduling future intentions ("reminders") that automatically surface in chat context when due.

```

latency 1468 ms; context hits STM=2 LTM=1; salience=0.48; ### Capabilities

valence=0.35; importance=0.52; consolidation: stored- Add reminders with relative due time (seconds from now)

```- List all / only pending reminders

- Retrieve due reminders (one-shot triggering)

---- Automatic injection of due reminders into chat context (rank 0) for explainability



## üèóÔ∏è Architecture Overview### In-Memory Model

Implemented as a fast, non-persistent singleton (`ProspectiveMemory`) distinct from the vector-based persistent system (which remains intact for future expansion). Each reminder:

### **Memory Systems**```

{

#### **Short-Term Memory (STM)**  "id": "uuid",

- **Implementation**: `VectorShortTermMemory` with ChromaDB  "content": "Send weekly report",

- **Capacity**: 7 items (Miller's magical number)  "due_ts": <epoch_seconds>,

- **Activation Model**: Recency + frequency + salience weighting  "due_in_seconds": <float>,

- **Decay**: Biologically-realistic forgetting curve  "created_ts": <epoch_seconds>,

- **Eviction**: LRU (Least Recently Used)  "triggered_ts": <epoch_seconds|null>,

  "metadata": { }

#### **Long-Term Memory (LTM)**}

- **Implementation**: `VectorLongTermMemory` with ChromaDB```

- **Storage**: Persistent vector embeddings

- **Retrieval**: Semantic similarity search### Injection Behavior

- **Decay**: Gradual importance degradation over timeOn each chat turn, any newly due reminders (not previously triggered) are:

- **Clustering**: Automatic semantic grouping1. Marked triggered (single-shot semantics)

2. Counted in metrics

#### **Consolidation Pipeline**3. Pushed into context items with fields:

``````

User Input ‚Üí Salience Scoring ‚Üí STM Storage Decision{

                                      ‚Üì  "source_system": "prospective",

                                 Rehearsal Tracking  "source_id": <reminder id>,

                                      ‚Üì  "reason": "due_reminder",

                              Dream Cycle / Promotion  "content": <reminder content>,

                                      ‚Üì  "rank": 0

                                  LTM Storage}

``````



**Consolidation Criteria:**### API Endpoints

- **Salience threshold**: User-adjustable via slider (default 0.55)```

- **Valence threshold**: Emotional intensity ‚â• 0.60POST /agent/reminders

- **Rehearsal count**: Referenced ‚â• 2 times  body: { "content": "Water the plants", "due_in_seconds": 300 }

- **Age requirement**: Exists ‚â• 5 seconds  -> 201 { reminder payload }

- **Importance floor**: Base importance ‚â• 0.4

GET /agent/reminders

### **Core Components**  -> 200 [ reminder payloads including triggered ]



#### **ChatService** (`src/chat/chat_service.py`)GET /agent/reminders/due

- Orchestrates chat turns and memory capture  -> 200 [ newly due (one-shot) reminders triggered at request time ]

- Manages consolidation decisions```

- Tracks performance metrics

### Metrics

#### **ContextBuilder** (`src/chat/context_builder.py`)New counters exposed via metrics registry:

- Retrieval pipeline: STM ‚Üí LTM ‚Üí Episodic ‚Üí Fallback```

- Scoring and ranking of context itemsprospective_reminders_created_total    # Incremented on POST create

- Timeout management for degraded performanceprospective_reminders_triggered_total  # Incremented when a reminder becomes due (either via /due or chat turn)

prospective_reminders_injected_total   # Incremented when due reminders are injected into chat context

#### **MemoryConsolidator** (`src/memory/consolidation/consolidator.py`)```

- STM storage with threshold gating

- Rehearsal tracking for promotion### Design Notes & Next Steps

- Age and frequency-based promotion to LTM- Keeps heavy vector ProspectiveMemorySystem untouched; future merge will unify persistence & semantic search.

- Current beta focuses on deterministic scheduling and visibility for turn-level reasoning.

#### **CognitiveAgent** (`src/core/cognitive_agent.py`)- Planned: persistence, natural-language scheduling ("in 5 minutes"), recurring reminders, promotion to LTM upon completion.

- Integrates memory, attention, and processing subsystems

- Provides unified interface to memory systems---

- Manages cognitive state and fatigue### Promotion Provenance

Promoted LTM items now carry a `promoted_from_stm` provenance flag (appears in:

### **Chat Pipeline Flow**1. Context item scores (`promoted_from_stm: 1.0`)

```2. Provenance details trace (`trace.provenance_details[].promoted_from_stm`)

1. User Input

   ‚ÜìExample provenance entry:

2. Salience & Valence Estimation```json

   ‚Üì{

3. Memory Capture (facts/preferences/goals)  "source_id": "ltm-turn-abc123",

   ‚Üì  "source_system": "ltm",

4. Context Retrieval (STM ‚Üí LTM ‚Üí Episodic ‚Üí Fallback)  "reason": "semantic_match",

   ‚Üì  "composite": 0.8421,

5. Attention Allocation & Scoring  "factors": [

   ‚Üì    {"factor": "similarity", "weight": 0.4, "value": 0.91, "contribution": 0.364, "category": "retrieval"},

6. LLM Generation with Context    {"factor": "activation", "weight": 0.3, "value": 0.73, "contribution": 0.219, "category": "retrieval"},

   ‚Üì    {"factor": "recency", "weight": 0.2, "value": 0.55, "contribution": 0.110, "category": "retrieval"},

7. Consolidation Decision (store in STM if threshold met)    {"factor": "salience", "weight": 0.1, "value": 0.49, "contribution": 0.049, "category": "retrieval"}

   ‚Üì  ],

8. Response with Metrics  "promoted_from_stm": true,

```  "composite_vs_factor_sum_delta": 0.0001

}

---```



## üéØ Key Features### Age & Rehearsal Gating

Promotion requires simultaneously:

### **1. Dynamic Memory Control**- Rehearsals >= policy.min_rehearsals_for_promotion

- **Real-time threshold adjustment**: Change consolidation criteria without code changes- Age (seconds since first seen) >= policy.min_age_seconds

- **Transparent scoring**: See exact salience/valence/importance values

- **Consolidation visibility**: Know when and why memories are storedThese safeguards prevent premature promotion and make the promotion age histogram meaningful.



### **2. Cognitive Realism**### Operational Uses

- **7-item STM capacity**: Based on cognitive psychology research- `promotion_age_p95_seconds` provides a stability signal (rising values may indicate lowered rehearsal frequency or throttled promotions)

- **Activation decay**: Realistic forgetting curves- `stm_store_total / ltm_promotions_total` ratio approximates consolidation selectivity

- **Rehearsal effects**: Repeated mentions strengthen memories- Provenance flag allows downstream explanation layers to highlight durable memories.

- **Age-gating**: Prevents premature consolidation

---

### **3. Production Quality**### API Schema (Performance & Consolidation)

- **Thread-safe**: Proper locking for concurrent operations

- **Error handling**: Comprehensive exception management with fallbacksMinimal OpenAPI-style fragments for new observability endpoints:

- **Performance monitoring**: P95 latency tracking, throughput metrics

- **Adaptive behavior**: Adjusts retrieval limits under load```yaml

paths:

### **4. Explainability**  /agent/chat/performance:

- **Full provenance**: Track memory source (STM/LTM/recent/attention/executive)    get:

- **Scoring breakdown**: See why items were selected for context      summary: Chat performance & consolidation metrics

- **Consolidation status**: "stored" vs "skipped" with reasons      responses:

- **Debug metrics**: Complete visibility into decision-making        '200':

          description: Performance snapshot

---          content:

            application/json:

## üì° API Endpoints              schema:

                type: object

### **Chat Endpoints** (`/agent/*`)                properties:

                  latency_p95_ms: { type: number }

#### `POST /agent/chat`                  performance_degraded: { type: boolean }

Primary chat interface with memory retrieval and consolidation.                  ema_turn_latency_ms: { type: number }

                  chat_turns_per_sec: { type: number }

**Request:**                  consolidation:

```json                    type: object

{                    properties:

  "message": "I went to the mall yesterday",                      counters:

  "session_id": "user123",                        type: object

  "consolidation_salience_threshold": 0.35,                        properties:

  "flags": {                          stm_store_total: { type: integer }

    "include_memory": true,                          ltm_promotions_total: { type: integer }

    "include_attention": true,                      promotion_age_p95_seconds: { type: number }

    "include_trace": false,                      selectivity_ratio: { type: number }

    "reflection": false                      recent_promotion_age_seconds:

  }                        type: object

}                        properties:

```                          count: { type: integer }

                          avg: { type: number }

**Response:**                          values:

```json                            type: array

{                            items: { type: number }

  "response": "That sounds great! How was your experience?",                      promotion_age_alert: { type: boolean }

  "context_items": [                      promotion_age_alert_threshold: { type: number }

    {  /agent/chat/consolidation/status:

      "source_system": "stm",    get:

      "content": "User enjoys shopping",      summary: Consolidation subsystem status

      "rank": 1,      responses:

      "reason": "high_activation",        '200':

      "scores": {"composite": 0.82}          description: Current consolidation counters and recent events

    }```

  ],

  "captured_memories": [

    {## üß† Complete STM & Attention Integration (July 2025)

      "content": "User visited mall",

      "memory_type": "fact",### Production-Grade Short-Term Memory System

      "frequency": 1The Short-Term Memory (STM) system has been completely modernized with a robust, production-grade implementation:

    }

  ],#### **Vector-Based STM with ChromaDB**

  "metrics": {- **VectorShortTermMemory**: Production-grade STM using ChromaDB vector database for semantic storage

    "turn_latency_ms": 1450,- **Capacity Management**: Biologically-inspired 7-item capacity with LRU eviction

    "stm_hits": 2,- **Activation-Based Decay**: Realistic forgetting mechanism based on recency, frequency, and salience

    "ltm_hits": 1,- **Semantic Retrieval**: Vector embeddings enable meaning-based memory search

    "user_salience": 0.48,- **Type Safety**: Full type annotations with comprehensive validation and error handling

    "user_valence": 0.12,

    "user_importance": 0.52,#### **Integrated Attention Mechanism**

    "consolidation_status": "stored"- **Active Attention Allocation**: Real attention mechanism integrated into cognitive processing pipeline

  }- **Neural Enhancement**: DPAD neural network provides attention boosts (+0.200 enhancement)

}- **Cognitive Load Tracking**: Real-time monitoring of fatigue, cognitive load, and attention capacity

```- **Focus Management**: Tracks attention items, switches, and available processing capacity

- **Biologically Realistic**: Fatigue accumulation, attention recovery, and capacity limits

#### `POST /agent/dream/start`

Trigger STM ‚Üí LTM consolidation cycle.#### **Core Architecture Improvements**

- **Unified Configuration**: Centralized `MemorySystemConfig` dataclass for consistent system configuration

**Request:**- **Robust Error Handling**: Comprehensive exception hierarchy with `VectorSTMError`, `MemorySystemError`, and specialized exceptions

```json- **Thread Safety**: Full thread-safe operations with proper locking mechanisms and connection pooling

{- **Input Validation**: Comprehensive input validation with detailed error messages

  "cycle_type": "light"- **Logging**: Structured logging with performance monitoring and operation tracking

}

```#### **Cognitive Agent Processing Pipeline**

The main cognitive processing loop now includes full STM and attention integration:

**Response:**

```json1. **Sensory Processing**: Raw input processed through entropy/salience scoring

{2. **Memory Retrieval**: Proactive recall searches both STM and LTM for context

  "dream_results": {3. **Attention Allocation**: Neural-enhanced attention with cognitive load tracking

    "cycle_type": "light",4. **Response Generation**: LLM integration with memory context and attention weighting

    "promoted_count": 3,5. **Memory Consolidation**: Interaction storage in STM with importance-based routing

    "method": "consolidator_fallback"6. **Cognitive State Update**: Real-time fatigue, attention focus, and efficiency tracking

  }

}#### **STM-Specific Features**

```- **ChromaDB Integration**: Persistent vector storage with embedding-based similarity search

- **Memory Item Structure**: Rich metadata including importance, attention scores, emotional valence

#### `GET /agent/chat/performance`- **Proactive Recall**: Context-aware memory search using conversation history

Retrieve performance and consolidation metrics.- **Capacity Enforcement**: Automatic LRU eviction when 7-item limit reached

- **Activation Calculation**: Sophisticated scoring based on recency, frequency, and salience

**Response:**- **Associative Search**: Direct association-based memory retrieval

```json

{#### **Attention Mechanism Features**

  "latency_p95_ms": 1850.5,- **Real-Time Allocation**: Dynamic attention distribution based on novelty, priority, and effort

  "performance_degraded": false,- **Neural Enhancement**: DPAD network provides consistent +0.200 attention boosts

  "chat_turns_per_sec": 0.42,- **Focus Tracking**: Maintains list of items currently in attentional focus

  "consolidation": {- **Cognitive Load Management**: Monitors processing capacity and available resources

    "counters": {- **Fatigue Modeling**: Realistic attention fatigue with recovery mechanisms

      "stm_store_total": 45,- **Rest Functionality**: Cognitive breaks to reduce fatigue and restore capacity

      "ltm_promotions_total": 12

    },#### **Enhanced API Design**

    "promotion_age_p95_seconds": 120.5- **Result Objects**: Structured `MemoryOperationResult` and `ConsolidationStats` for detailed operation feedback

  }- **Protocol-Based Design**: Type-safe protocols for `MemorySearchable` and `MemoryStorable` interfaces

}- **Lazy Loading**: Memory systems initialized on-demand for improved startup performance

```- **Comprehensive Status**: Detailed system status with uptime, operation counts, and configuration



#### Other Endpoints#### **Prospective Memory Evolution**

- `GET /agent/chat/consolidation/status` - Consolidation subsystem status- **Persistent Vector Storage**: ChromaDB-based persistent storage with GPU-accelerated embeddings

- `GET /agent/chat/preview` - Preview context without generation- **Semantic Search**: Advanced semantic search capabilities for finding related intentions

- `GET /agent/chat/metacog/status` - Metacognitive snapshot- **Automatic Migration**: Due reminders automatically migrate to LTM with outcome tracking

- `POST /agent/reminders` - Create prospective memory reminders- **API Integration**: RESTful API endpoints for reminder management and processing

- `GET /agent/reminders/due` - Check due reminders



---### **STM & Attention Integration Results**

Based on comprehensive testing, the integrated system demonstrates:

## üîß Configuration

- **Perfect Reliability**: 0.0% error rate with 13+ operations in testing

### **Core Settings** (`src/core/config.py`)- **Biologically Realistic**: 7-item STM capacity with realistic activation patterns

- **Neural Enhancement**: Consistent +0.200 attention boosts from DPAD network

#### **ChatConfig**- **Semantic Storage**: Vector embeddings enable meaning-based memory retrieval

```python- **Cognitive Load Tracking**: Real-time monitoring of attention capacity (0.000 ‚Üí 0.862 observed)

class ChatConfig:- **Proactive Recall**: Context-aware memory search using conversation history

    max_recent_turns: int = 8- **Automatic Management**: LRU eviction and activation-based decay working correctly

    max_context_items: int = 16- **Memory Consolidation**: Each interaction properly stored with attention weighting

    stm_activation_min: float = 0.15

    ltm_similarity_threshold: float = 0.62### **Production Features**

    consolidation_salience_threshold: float = 0.55  # Adjustable via API- **Resource Management**: Proper cleanup and shutdown procedures with ChromaDB connection management

    consolidation_valence_threshold: float = 0.60- **Health Monitoring**: System health checks and diagnostic reporting for both STM and attention

    performance_target_p95_ms: int = 1000- **Performance Optimization**: Connection pooling, caching, and efficient memory usage

```- **Security**: Input sanitization and validation throughout the STM system

- **Monitoring**: Comprehensive metrics and logging for production deployment

#### **MemoryConfig**- **Type Safety**: Full type annotations with `VectorShortTermMemory`, `MemoryItem`, and `AttentionMechanism`

```python

class MemoryConfig:---

    stm_capacity: int = 7  # Miller's magical number

    stm_decay_threshold: float = 0.1## üß† Enhanced Long-Term Memory with Biologically-Inspired Features (June 2025)

    ltm_similarity_threshold: float = 0.7

    consolidation_interval_hours: int = 8### Advanced LTM Capabilities

```The Long-Term Memory (LTM) system has been significantly enhanced with biologically-inspired features that mirror human memory processes:



### **Environment Variables** (`.env`)#### 1. Salience & Recency Weighting in Retrieval

```bash- **Dynamic Retrieval Scoring**: Memory retrieval now considers both content relevance and temporal/access patterns

# Required- **Exponential Decay Model**: Recent and frequently accessed memories receive higher priority in search results

OPENAI_API_KEY=sk-your-key-here- **Access Pattern Learning**: System learns which memories are most valuable based on usage patterns



# Optional - ChromaDB Configuration#### 2. Memory Decay & Forgetting

CHROMA_PERSIST_DIR=./data/memory_stores/chroma- **Biological Forgetting Curves**: Implements Ebbinghaus-style forgetting with configurable decay rates

STM_COLLECTION=stm_collection- **Importance-Based Preservation**: More important memories resist decay longer

LTM_COLLECTION=ltm_collection- **Confidence Degradation**: Memory confidence naturally decreases over time without reinforcement

- **Selective Pruning**: Old, rarely accessed memories automatically lose strength

# Optional - Performance Tuning

DISABLE_SEMANTIC_MEMORY=0  # Set to 1 to skip heavy semantic ops#### 3. Consolidation Tracking

```- **STM‚ÜíLTM Transfer Monitoring**: Tracks when and how memories move from short-term to long-term storage

- **Consolidation Metadata**: Records consolidation timestamps, sources, and transfer statistics

---- **Query Methods**: Retrieve recently consolidated memories and analyze consolidation patterns

- **Performance Analytics**: Detailed statistics on memory consolidation efficiency

## üß™ Testing

#### 4. Meta-Cognitive Feedback

### **Run All Tests**- **Self-Monitoring**: System tracks its own memory performance and retrieval patterns

```bash- **Health Diagnostics**: Automatic assessment of memory system health and performance

pytest -q- **Usage Statistics**: Comprehensive metrics on search success rates, timing, and efficiency

```- **Recommendations Engine**: System provides suggestions for memory management optimization



### **Test Specific Components**#### 5. Emotionally Weighted Consolidation

```bash- **Emotional Significance**: Memories with strong emotional content (positive or negative) are prioritized for consolidation

# Chat integration- **Multi-Factor Scoring**: Combines importance, access frequency, emotional weight, and recency for consolidation decisions

pytest tests/test_chat_factory_integration.py -v- **Adaptive Thresholds**: Emotional memories may be consolidated even with lower traditional importance scores

- **Trauma/Joy Preservation**: Both traumatic and highly positive experiences receive enhanced consolidation

# Memory consolidation

pytest tests/test_chat_consolidation_thresholds.py -v#### 6. Cross-System Query & Linking

- **Bidirectional Associations**: Create and query links between LTM and other memory systems (STM, episodic)

# STM/LTM retrieval- **Semantic Clustering**: Automatically identify and group related memories by content and tags

pytest tests/test_chat_adaptive_retrieval.py -v- **Cross-System Suggestions**: AI-powered recommendations for linking memories across different systems

- **Association Networks**: Build rich networks of related memories for enhanced recall and context

# Salience scoring

pytest tests/test_chat_emotion_salience_ranking.py -v### Testing & Validation

```- **Comprehensive Test Suite**: Individual tests for each enhanced feature

- **Integration Testing**: End-to-end testing of all features working together

### **Coverage Report**- **Performance Benchmarks**: Validation of enhanced retrieval speed and accuracy

```bash- **Biological Validation**: Tests confirm human-like memory behavior patterns

pytest --cov=src --cov-report=html

```### Key Benefits

- **Human-Like Memory**: More realistic forgetting and remembering patterns

---- **Improved Efficiency**: Better memory management through automated decay and consolidation

- **Enhanced Recall**: Smarter retrieval based on usage patterns and emotional significance

## üìÅ Project Structure- **Self-Optimization**: System continuously improves its own memory management

- **Rich Associations**: Better context and relationship understanding across memories

```

human_ai_local/---

‚îú‚îÄ‚îÄ src/

‚îÇ   ‚îú‚îÄ‚îÄ chat/                      # Chat service & pipeline## üöÄ Recent Major Update: Unified Memory Interface & Episodic Memory Improvements (June 2025)

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat_service.py        # Main orchestrator

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context_builder.py     # Retrieval pipeline### Unified Memory Interface

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emotion_salience.py    # Salience/valence scoring- **All major memory modules (STM, LTM, Episodic, Semantic)** now implement a consistent, type-safe interface via a shared `BaseMemorySystem` class (`src/memory/base.py`).

‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scoring.py             # Context item ranking- **Unified API methods** for all memory systems:

‚îÇ   ‚îú‚îÄ‚îÄ memory/                    # Memory subsystems  - `store(...)`: Store a new memory (returns memory ID)

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stm/                   # Short-term memory  - `retrieve(memory_id)`: Retrieve a memory as a dict

‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector_stm.py      # Vector-based STM  - `delete(memory_id)`: Delete a memory by ID

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ltm/                   # Long-term memory  - `search(query, **kwargs)`: Search for memories (returns list of dicts)

‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector_ltm.py      # Persistent LTM- All memory modules return dicts and use unified parameter names for easier integration and testing.

‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ consolidation/         # STM ‚Üí LTM pipeline

‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ consolidator.py    # Promotion logic### Episodic Memory System Enhancements

‚îÇ   ‚îú‚îÄ‚îÄ interfaces/api/            # REST API- **Fallback search**: Robust fallback logic using word overlap heuristics and debug output if ChromaDB is unavailable or returns no results.

‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat_endpoints.py      # Chat & dream endpoints- **Related memory logic**: Improved detection of related memories (temporal, cross-reference, semantic) with debug output for explainability.

‚îÇ   ‚îú‚îÄ‚îÄ core/                      # Core configuration- **New/Updated Public API Methods**:

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py              # System configuration  - `get_related_memories(memory_id, relationship_types=None, limit=10)`

‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cognitive_agent.py     # Main agent class  - `get_autobiographical_timeline(life_period=None, start_date=None, end_date=None, limit=50)`

‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent_singleton.py     # Singleton instance  - `consolidate_memory(memory_id, strength_increment=0.1)`

‚îÇ   ‚îî‚îÄ‚îÄ attention/                 # Attention mechanisms  - `get_memory_statistics()`

‚îÇ       ‚îî‚îÄ‚îÄ attention_mechanism.py # Fatigue & focus  - `clear_memory(older_than=None, importance_threshold=None)`

‚îú‚îÄ‚îÄ scripts/  - `get_consolidation_candidates(min_importance=0.5, max_consolidation=0.9, limit=10)`

‚îÇ   ‚îî‚îÄ‚îÄ george_streamlit_chat.py   # Minimal chat UI  - `clear_all_memories()` (for test isolation)

‚îú‚îÄ‚îÄ tests/                         # Comprehensive test suite- **Debug output**: All fallback and related memory logic now prints detailed debug information for transparency and troubleshooting.

‚îú‚îÄ‚îÄ start_server.py                # API server startup

‚îú‚îÄ‚îÄ start_george.py                # Combined startup script### Testing & Reliability

‚îî‚îÄ‚îÄ requirements.txt               # Python dependencies- **Integration tests** for vector LTM and episodic memory updated to use the new interface and auto-generated summaries.

```- **Test isolation**: All tests clear persistent and in-memory data before each run for clean, isolated test runs.

- **All episodic memory integration tests pass.**

---

---

## üé® Customization Guide

## Unified Memory Interface and Episodic Memory Enhancements (June 2025)

### **Adjust Consolidation Thresholds**

### Unified Memory Interface

**Option 1: Via UI Slider** (Recommended)- All major memory modules (STM, LTM, Episodic, Semantic) now implement a consistent, type-safe interface via a shared `BaseMemorySystem` class (`src/memory/base.py`).

- Open chat interface at http://localhost:8501- Unified public API methods: `store`, `retrieve`, `delete`, `search` (all return dicts, use unified parameter names).

- Adjust slider in sidebar: "STM Consolidation Threshold"- All memory modules inherit from `BaseMemorySystem` and enforce type annotations for robust, modular design.

- Changes apply immediately to all new messages

### Episodic Memory System Improvements

**Option 2: Via Code** (Persistent)- Major refactor of `EpisodicMemorySystem` (`src/memory/episodic/episodic_memory.py`):

Edit `src/core/config.py`:  - Robust fallback search logic (word overlap, text match) with detailed debug output for explainability.

```python  - Enhanced related memory logic (cross-reference, temporal, semantic) with debug output.

@dataclass  - All required public API methods implemented and exposed:

class ChatConfig:    - `get_related_memories`

    consolidation_salience_threshold: float = 0.35  # Lower = more memories    - `get_autobiographical_timeline`

    consolidation_valence_threshold: float = 0.50   # Emotional intensity    - `consolidate_memory`

```    - `get_memory_statistics`

    - `clear_memory`

### **Modify Salience Calculation**    - `get_consolidation_candidates`

    - `clear_all_memories` (for test isolation)

Edit `src/chat/emotion_salience.py`:- All methods return type-safe results and provide debug output for fallback/related memory logic.

```python

def estimate_salience_and_valence(text: str) -> Tuple[float, float]:### Testing and Test Isolation

    # Adjust weights:- Integration tests for vector LTM and episodic memory updated to use the new interface.

    # - length_factor weight (currently 0.4)- Test isolation: persistent and in-memory data cleared before each run to ensure clean test runs.

    # - punctuation_boost (0.07 per ! or ?)- All episodic memory integration tests pass.

    # - uppercase_ratio boost (0.6)

    # - intensifier_boost (0.05 per word)### Documentation

    - This section documents the new unified memory interface, episodic memory improvements, new public API methods, and updated testing strategy.

    salience = max(0.0, min(1.0,- See `src/memory/base.py` for the base interface and `src/memory/episodic/episodic_memory.py` for the full implementation and debug logic.

        0.25 +                        # Base salience

        length_factor * 0.4 +         # Adjust this weight---

        punctuation_boost +

        emphasis_boost +## üöÄ Major Update: Unified Memory, Procedural Memory, and CLI Integration (June 2025)

        intensifier_boost

    ))### Procedural Memory System

```- **ProceduralMemory** is now fully integrated with STM and LTM. Procedures (skills, routines, action sequences) can be stored as either short-term or long-term memories.

- Unified API: Store, retrieve, search, use, delete, and clear procedural memories via the same interface as other memory types.

### **Add Custom Memory Types**- **Persistence:** Procedures stored in LTM are persistent across runs; STM procedures are in-memory only.

- **Tested:** Comprehensive tests ensure correct storage, retrieval, and deletion from both STM and LTM.

Extend `src/chat/memory_capture.py` to extract new types:

```python### CLI Integration

# Add to MemoryCapture._extract_memories()- The `george_cli.py` script now supports procedural memory management:

if "deadline" in text.lower() or "due" in text.lower():  - `/procedure add` ‚Äî interactively add a new procedure (description, steps, tags, STM/LTM)

    memories.append(CapturedMemory(  - `/procedure list` ‚Äî list all stored procedures

        content=text,  - `/procedure search <query>` ‚Äî search procedures by description/steps

        memory_type="deadline",  - `/procedure use <id>` ‚Äî increment usage and display steps for a procedure

        importance=0.7  - `/procedure delete <id>` ‚Äî delete a procedure by ID

    ))  - `/procedure clear` ‚Äî remove all procedural memories

```

### Metacognitive Reflection & Self-Monitoring (June 2025)

### **Customize Dream Cycle Behavior**- **Agent-level self-reflection:** The agent can periodically or manually analyze its own memory health, usage, and performance.

- **Reflection Scheduler:** Background scheduler runs metacognitive reflection at a configurable interval (default: 10 min).

Edit `src/memory/consolidation/consolidator.py`:- **Manual Reflection:** Trigger a reflection at any time via CLI or API.

```python- **Reporting:** Reflection reports include LTM/STM stats, health diagnostics, and recommendations for memory management.

@dataclass- **CLI Integration:**

class ConsolidationPolicy:  - `/reflect` ‚Äî manually trigger a reflection and print summary

    min_rehearsals_for_promotion: int = 1  # Lower = faster promotion  - `/reflection status` ‚Äî show last 3 reflection reports

    min_age_seconds: float = 2.0           # Lower = immediate promotion  - `/reflection start [interval]` ‚Äî start scheduler (interval in minutes)

    promotion_importance_floor: float = 0.3 # Lower = more permissive  - `/reflection stop` ‚Äî stop scheduler

```



------



## üîç Troubleshooting

## API Endpoints (Selected)

### **No STM Hits Even with Low Threshold**

- **Check debug metrics**: Look at `user_salience` value in responseCore chat & cognition service endpoints (FastAPI):

- **Verify threshold**: Ensure slider value < calculated salience

- **Check consolidation status**: Should show "stored" not "skipped"- `POST /agent/chat` ‚Äì Process a chat message (optional streaming via `stream=true`)

- **Restart server**: Changes to code require backend restart- `GET /agent/chat/preview` ‚Äì Deterministic context preview (no generation)

- `GET /agent/chat/metrics` ‚Äì Metrics snapshot (light by default)

### **Memories Not Promoting to LTM**- `GET /agent/chat/performance` ‚Äì Performance status (latency p95, degradation flag)

- **Trigger dream cycle**: Click üåô button or wait for automatic promotion- `GET /agent/chat/consolidation/status` ‚Äì Consolidation subsystem status, counters, recent events (inactive flag if not configured)

- **Check rehearsal count**: Memory needs to be referenced ‚â•2 times

- **Verify age**: Memory must exist for ‚â•5 seconds## CLI Commands

- **Check results**: Dream cycle shows `promoted_count` in response

### **Memory Operations**

### **High Latency / Slow Responses**```bash

- **Check STM utilization**: When >85%, system throttles consolidation# Memory management

- **Reduce context items**: Lower `max_context_items` in config/memory store <system> <content>     # Store memory in STM/LTM

- **Enable fallback**: System automatically uses degraded mode if needed/memory search <system> <query>      # Search memories

- **Monitor metrics**: Check `/agent/chat/performance` endpoint/memory list <system>                # List all memories

/memory retrieve <system> <id>       # Retrieve specific memory

### **ChromaDB Connection Errors**/memory delete <system> <id>         # Delete memory

- **Check persistence directory**: Ensure `CHROMA_PERSIST_DIR` exists

- **Permissions**: Verify write access to data directory# Procedural memory

- **Clean slate**: Delete `data/memory_stores/chroma/` and restart/procedure add                       # Add new procedure interactively

/procedure list                      # List all procedures

### **Backend Not Starting**/procedure search <query>            # Search procedures

```bash/procedure use <id>                  # Use procedure (increment usage)

# Check if port 8000 is in use/procedure delete <id>               # Delete procedure by ID

netstat -ano | findstr :8000  # Windows/procedure clear                     # Remove all procedural memories

lsof -i :8000                 # Linux/Mac

# Prospective memory (reminders)

# Kill existing process if needed/remind me to <task> at <YYYY-MM-DD HH:MM>

# Then restart/remind me to <task> in <minutes> minutes

python start_server.py/reminders                           # List reminders

```/reminders process                   # Process due reminders



---# Metacognitive reflection

/reflect                            # Trigger manual reflection

## üìö Additional Documentation/reflection status                   # Show reflection status

/reflection start [interval]         # Start reflection scheduler

- **`docs/ai.instructions.md`**: AI agent development guidelines/reflection stop                     # Stop reflection scheduler

- **`docs/metacog_features.md`**: Metacognitive system details```

- **`docs/roadmap.md`**: Future development plans

- **`STARTUP_GUIDE.md`**: Detailed startup instructions### **API Endpoints**

- **`CLEANUP_SUMMARY.md`**: Recent refactoring changes```bash

- **`.github/copilot-instructions.md`**: Codebase working rules# Memory operations

POST /memory/store                   # Store memory

---GET /memory/search                   # Search memories

GET /memory/status                   # Get system status

## ü§ù Contributing

# Executive functions

### **Development Workflow**POST /api/executive/goals            # Create a new goal

1. Fork the repositoryGET /api/executive/goals             # List all goals

2. Create a feature branch: `git checkout -b feature/my-feature`GET /api/executive/goals/{id}        # Get specific goal details

3. Make changes with testsPUT /api/executive/goals/{id}        # Update a goal

4. Run test suite: `pytest -q`DELETE /api/executive/goals/{id}     # Delete a goal

5. Run linter: `ruff check src/`POST /api/executive/tasks            # Create tasks for a goal

6. Submit pull requestGET /api/executive/tasks             # List all tasks

GET /api/executive/tasks/{id}        # Get specific task details

### **Code Standards**PUT /api/executive/tasks/{id}        # Update task status

- **Python 3.12+**: Use modern Python featuresPOST /api/executive/decisions        # Make a decision

- **Type hints**: Full type annotations requiredGET /api/executive/decisions/{id}    # Get decision details

- **Docstrings**: All public functions documentedGET /api/executive/resources         # Get resource allocation status

- **Tests**: New features require test coveragePOST /api/executive/resources/allocate  # Allocate cognitive resources

- **Linting**: Pass `ruff` checks before committingGET /api/executive/status            # Get comprehensive executive status

POST /api/executive/reflect          # Trigger executive reflection

---GET /api/executive/performance       # Get performance metrics



## üìÑ License# Prospective memory

POST /prospective/store              # Add reminder

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.GET /prospective/due                 # Get due reminders

POST /prospective/process_due        # Process due reminders

---

# Agent interaction

## üôè AcknowledgmentsPOST /agent/chat                     # Chat with agent

GET /agent/status                    # Get agent status

- **OpenAI**: GPT-4 language modelGET /agent/chat/performance          # Chat performance status (p95, target, degraded)

- **ChromaDB**: Vector database infrastructure# System management

- **Hugging Face**: Sentence transformersPOST /test/reset                     # Reset system (test only)

- **Streamlit**: Interactive UI frameworkGET /health                          # Health check

- **FastAPI**: Modern web framework```

- **PyTorch**: Neural network components

## Usage Example (Unified Memory API)

---```python

# Example: Storing and searching episodic memory

## üìû Supportfrom src.memory.episodic.episodic_memory import EpisodicMemorySystem



- **GitHub Issues**: Report bugs or request featuresmemsys = EpisodicMemorySystem()

- **Documentation**: Check `docs/` directory for detailed guidesmem_id = memsys.store(detailed_content="Visited the science museum with friends.")

- **API Docs**: Visit http://localhost:8000/docs when server is runningresult = memsys.retrieve(mem_id)

print(result)

---

# Search

**Human-AI Cognition Framework** - Building transparent, biologically-inspired AI with human-like memory and reasoning.results = memsys.search(query="museum")

for r in results:

*Version 2.1.0 (October 2025) - Interactive Memory Control Update*    print(r)

```

## Executive API Usage Examples
```bash
# Create a strategic goal
curl -X POST http://localhost:8000/api/executive/goals \
  -H "Content-Type: application/json" \
  -d '{
    "description": "Build a web application",
    "priority": 0.8,
    "deadline": "2025-12-31"
  }'

# List all active goals
curl http://localhost:8000/api/executive/goals?active_only=true

# Create tasks for a goal
curl -X POST http://localhost:8000/api/executive/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "goal_id": "goal_123",
    "description": "Design user interface",
    "priority": 0.7,
    "estimated_effort": 8.0
  }'

# Make a strategic decision
curl -X POST http://localhost:8000/api/executive/decisions \
  -H "Content-Type: application/json" \
  -d '{
    "context": "Which programming language to use?",
    "options": ["Python", "JavaScript", "Go"],
    "criteria": {"speed": 0.3, "ease": 0.4, "ecosystem": 0.3}
  }'

# Get executive system status
curl http://localhost:8000/api/executive/status

# Allocate cognitive resources
curl -X POST http://localhost:8000/api/executive/resources/allocate \
  -H "Content-Type: application/json" \
  -d '{
    "resource_demands": {"attention": 0.8, "memory": 0.6, "processing": 0.7},
    "priority": 0.9,
    "duration_minutes": 30
  }'

# Trigger executive reflection
curl -X POST http://localhost:8000/api/executive/reflect
```

---


## Core Architecture

### Memory Systems
- **Short-Term Memory (STM)**: In-memory, time-decayed, vector search (ChromaDB), 7-item capacity, LRU eviction, activation-based decay, semantic retrieval, and proactive recall.
- **Long-Term Memory (LTM)**: ChromaDB vector database with salience/recency weighting, Ebbinghaus-style forgetting, consolidation tracking, meta-cognitive feedback, emotional weighting, and cross-system linking.
- **Episodic Memory**: ChromaDB vector DB with rich metadata, proactive recall, summarization/tagging, related memory logic, and timeline features.
- **Semantic Memory**: Structured factual knowledge (subject-predicate-object triples), persistent triple store, agent-level interface.
- **Prospective Memory**: Persistent reminders with semantic search and automatic migration.
- **Procedural Memory**: Skills/action sequences with unified STM/LTM storage, CLI/API support.

### Cognitive Processing
- **Attention Mechanism**: Salience/relevance weighting, fatigue modeling, neural enhancement (DPAD), cognitive load tracking, focus management, and rest/recovery.
- **Sensory Processing**: Multimodal input, entropy/salience scoring, attention allocation.
- **Meta-Cognition**: Self-reflection, memory management, health monitoring, and reporting.
- **Dream-State**: Background memory consolidation, clustering, optimization.
- **Neural Integration**: DPAD (Dual-Path Attention Dynamics), LSHN (Latent Structured Hopfield Networks), GPU acceleration.

### Production Features
- **Thread Safety**: Full concurrent operation with proper locking.
- **Error Handling**: Comprehensive exception hierarchy, graceful degradation.
- **Performance Monitoring**: Real-time metrics, operation tracking, health diagnostics.
- **Resource Management**: Cleanup, connection pooling, context managers, configuration management.
- **Configuration Management**: Centralized configuration with validation and defaults

## Technology Stack
- **Python 3.12**: Production-ready with full type hints and async support
- **OpenAI GPT-4.1**: Advanced language model integration
- **ChromaDB**: Vector database for persistent memory with GPU acceleration
- **sentence-transformers**: Semantic embedding generation
- **torch**: Neural network components and GPU acceleration
- **schedule/apscheduler**: Background task scheduling
- **threading/asyncio**: Concurrent processing and thread safety
- **dataclasses/protocols**: Type-safe configuration and interfaces

## Project Structure

```
human_ai_local/
‚îú‚îÄ‚îÄ src/                          # Main source code
‚îÇ   ‚îú‚îÄ‚îÄ core/                     # Core cognitive architecture
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration management
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cognitive_agent.py   # Main cognitive orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ memory/                   # Memory systems
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory_system.py     # Integrated memory coordinator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stm/                 # Short-term memory implementation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ltm/                 # Long-term memory with ChromaDB
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prospective/         # Future-oriented memory
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ procedural/          # Skills and procedures
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ consolidation/       # Memory consolidation pipeline
‚îÇ   ‚îú‚îÄ‚îÄ attention/               # Attention mechanisms
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ attention_mechanism.py # Advanced attention with fatigue modeling
‚îÇ   ‚îú‚îÄ‚îÄ processing/              # Cognitive processing layers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sensory/            # Sensory input processing with entropy scoring
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ neural/             # Neural network components
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lshn_network.py  # Latent Structured Hopfield Networks
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dpad_network.py  # Dual-Path Attention Dynamics
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ neural_integration.py # Neural integration manager
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dream/              # Dream-state consolidation processor
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings/         # Text embedding generation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clustering/         # Memory clustering algorithms
‚îÇ   ‚îú‚îÄ‚îÄ executive/              # Executive functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ goal_manager.py     # Hierarchical goal management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ task_planner.py     # Goal decomposition and task planning
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ decision_engine.py  # Multi-criteria decision making
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cognitive_controller.py # Resource allocation and cognitive state management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ executive_agent.py  # Central executive orchestrator
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ executive_models.py # Executive data structures and types
‚îÇ   ‚îú‚îÄ‚îÄ interfaces/             # External interfaces
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aws/               # AWS service integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ streamlit/         # Dashboard interface
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/               # REST API endpoints
‚îÇ   ‚îî‚îÄ‚îÄ utils/                  # Utility functions
‚îú‚îÄ‚îÄ tests/                      # Comprehensive test suites (30+ test files)
‚îÇ   ‚îú‚îÄ‚îÄ test_executive_system.py      # Executive functioning integration tests
‚îÇ   ‚îú‚îÄ‚îÄ test_memory_integration.py    # Memory system integration tests
‚îÇ   ‚îú‚îÄ‚îÄ test_dream_consolidation_pipeline.py # Dream processing tests
‚îÇ   ‚îú‚îÄ‚îÄ test_dpad_integration_fixed.py # DPAD neural network tests
‚îÇ   ‚îú‚îÄ‚îÄ test_lshn_integration.py      # LSHN neural network tests
‚îÇ   ‚îú‚îÄ‚îÄ test_attention_integration.py # Attention mechanism tests
‚îÇ   ‚îî‚îÄ‚îÄ test_final_integration_demo.py # Complete system demonstrations
‚îú‚îÄ‚îÄ data/                       # Data storage
‚îÇ   ‚îú‚îÄ‚îÄ memory_stores/         # ChromaDB vector databases
‚îÇ   ‚îú‚îÄ‚îÄ embeddings/            # Cached embeddings
‚îÇ   ‚îú‚îÄ‚îÄ models/                # Trained neural models (DPAD/LSHN)
‚îÇ   ‚îî‚îÄ‚îÄ exports/               # Data exports
‚îú‚îÄ‚îÄ docs/                       # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ ai.instructions.md     # Comprehensive development guide
‚îú‚îÄ‚îÄ config/                     # Configuration files
‚îú‚îÄ‚îÄ scripts/                    # Utility scripts
‚îú‚îÄ‚îÄ notebooks/                  # Jupyter notebooks
‚îú‚îÄ‚îÄ infrastructure/             # Infrastructure as Code
‚îú‚îÄ‚îÄ start_server.py            # Core API server startup
‚îú‚îÄ‚îÄ start_george.py            # Python launcher (all platforms)
‚îú‚îÄ‚îÄ start_george.sh            # Shell script (Git Bash/Linux/Mac)
‚îú‚îÄ‚îÄ STARTUP_README.md          # Startup instructions
‚îî‚îÄ‚îÄ STARTUP_GUIDE.md           # Detailed troubleshooting
```

## Quick Start

### **Installation**
```bash
# 1. Install dependencies (in your virtualenv):
pip install -r requirements.txt

# 2. Set up environment variables:
cp .env.example .env
# Edit .env with your OpenAI API key
```

### **Running the System**

#### **üöÄ Quick Start (Recommended)**
The fastest way to start George is with the startup scripts:
```bash
# Option 1: Git Bash / Linux / Mac
./start_george.sh

# Option 2: Any terminal
python start_george.py
```
These scripts automatically:
- ‚úÖ Detect your virtual environment
- ‚úÖ Start the API server (http://localhost:8000)
- ‚úÖ Launch the minimal Streamlit chat interface
- ‚úÖ Handle initialization progress
- ‚úÖ Open your browser automatically

#### ** Manual Startup (Advanced)**
```bash
# 1. Start the backend API server:
python start_server.py

# 2. In another terminal, start the Streamlit chat interface:
python -m streamlit run scripts/george_streamlit_chat.py --server.port 8501

# 3. Use the CLI interface (optional):
python scripts/george_cli.py
```

#### **üìç Access Points**
- **Chat Interface**: http://localhost:8501 (Streamlit minimal chat)
- **API Documentation**: http://localhost:8000/docs  
- **API Health Check**: http://localhost:8000/health

#### **üí¨ Chat Interface Features**
The minimal Streamlit interface (`george_streamlit_chat.py`) provides:
- **STM‚ÜíLTM‚ÜíLLM Pipeline**: Each chat first searches Short-Term Memory, then Long-Term Memory, then passes relevant context to the LLM
- **Context Visibility**: See which memory systems contributed to each response (stm/ltm/recent/attention/executive)
- **Captured Memories**: View what facts/preferences/goals the system extracted from your conversation
- **Performance Metrics**: Latency, STM/LTM hit counts, fallback status
- **Dream Cycle**: Trigger STM‚ÜíLTM consolidation via API endpoint (or adjust consolidation thresholds in `src/core/config.py`)

**Tip**: To capture everyday conversation in STM, lower `consolidation_salience_threshold` from 0.55 to ~0.35 in `ChatConfig`, or use emphatic language (caps, exclamation marks, emotionally-charged words) to cross default thresholds.

### **Testing the Executive System**
```bash
# Test complete executive functioning integration:
python -m pytest tests/test_executive_system.py -v

# Quick executive system demo:
python -c "
import asyncio
from src.executive.executive_agent import ExecutiveAgent

async def demo_executive():
    agent = ExecutiveAgent()
    
    # Test strategic planning
    goal_id = agent.goal_manager.create_goal(
        description='Build a web application',
        priority=0.8,
        deadline='2025-08-01'
    )
    
    # Test task planning
    tasks = agent.task_planner.create_tasks_for_goal(goal_id)
    
    # Test decision making
    decision = agent.decision_engine.make_decision(
        context='Which programming language to use?',
        options=['Python', 'JavaScript', 'Go'],
        criteria={'speed': 0.3, 'ease': 0.4, 'ecosystem': 0.3}
    )
    
    # Test resource management
    allocation = agent.cognitive_controller.allocate_resources({
        'attention': 0.8,
        'memory': 0.6, 
        'processing': 0.7
    })
    
    print(f'Goals: {len(agent.goal_manager.goals)}')
    print(f'Tasks: {len(tasks)}')
    print(f'Decision: {decision.selected_option} (confidence: {decision.confidence:.2f})')
    print(f'Resource allocation: {allocation}')
    
    await agent.shutdown()

asyncio.run(demo_executive())
"
```

## Streamlit Dashboard (George)
The production Streamlit interface provides a comprehensive cognitive architecture dashboard including:
- Enhanced chat interface with cognitive monitoring
- Real-time attention and memory system status
- Executive functioning controls and goal management
- Memory exploration across STM, LTM, episodic, and semantic systems
- Metacognitive reflection and system diagnostics

To launch:
```bash
# Use the startup scripts (recommended)
./start_george.sh
# or
python start_george.py

# Or manually start just the interface
python -m streamlit run scripts/george_streamlit_chat.py --server.port 8501
```
Then open [http://localhost:8501](http://localhost:8501) in your browser.



## Unique Features

### **Executive Functioning System**
- **Strategic Planning**: Hierarchical goal management with priority-based resource allocation
- **Multi-Criteria Decision Making**: Weighted decision analysis with confidence assessment
- **Task Decomposition**: Automated goal breakdown with dependency resolution
- **Cognitive Mode Management**: Dynamic switching between focused, multi-task, exploration modes
- **Resource Optimization**: Real-time allocation of attention, memory, and processing resources
- **Performance Monitoring**: Executive effectiveness tracking with adaptation recommendations

### **Production-Grade Architecture**
- **Thread-Safe Operations**: Full concurrent access with proper locking mechanisms
- **Comprehensive Error Handling**: Graceful degradation with detailed error reporting
- **Performance Monitoring**: Real-time metrics, operation tracking, and health diagnostics
- **Resource Management**: Proper cleanup, connection pooling, and context managers
- **Input Validation**: Comprehensive validation with sanitization and type checking

### **Biologically-Inspired Memory**
- **Realistic Forgetting Curves**: Ebbinghaus-style decay with importance-based preservation
- **Salience Weighting**: Temporal and access-pattern based retrieval prioritization
- **Emotional Consolidation**: Emotion-based memory consolidation and retention
- **Cross-System Linking**: Rich associative networks across different memory types
- **Meta-Cognitive Self-Monitoring**: System tracks and optimizes its own performance

### **Advanced Cognitive Features**
- **Attention Fatigue Modeling**: Realistic attention dynamics with decay and recovery
- **Dream-State Consolidation**: Background memory optimization and clustering
- **Proactive Memory Recall**: Context-aware memory activation and suggestion
- **Semantic Knowledge Integration**: Structured fact storage and retrieval
- **Prospective Memory Management**: Persistent reminders with semantic search

### **Neural Network Integration**
- **DPAD (Dual-Path Attention Dynamics)**: Advanced attention mechanism with parallel processing
- **LSHN (Latent Structured Hopfield Networks)**: Associative memory with structured patterns
- **GPU Acceleration**: CUDA-optimized embedding generation and neural processing
- **Vector Search**: ChromaDB-based semantic similarity search across all memory types

## Roadmap & Future Development

### **Immediate Priorities (Q3 2025)**

#### **Production-Ready Streamlit Interface Development**
**Phase 1 (4 weeks) - Critical User Experience Features:**
- **Enhanced Chat Interface**: Full cognitive integration with context awareness, reasoning display, and memory visualization
- **Memory Management Dashboard**: Multi-modal memory browser (STM/LTM/Episodic/Semantic/Procedural/Prospective) with health monitoring
- **Attention Monitor**: Real-time cognitive load visualization, fatigue tracking, and cognitive break controls
- **Basic Executive Dashboard**: Goal and task management interface with progress tracking

**Phase 2 (8 weeks) - Advanced Cognitive Features:**
- **Procedural Memory Interface**: Procedure builder, library browser, and execution monitoring
- **Prospective Memory Calendar**: Event scheduling, reminders, and due events dashboard
- **Neural Activity Monitor**: DPAD/LSHN network visualization and performance analytics
- **Performance Analytics**: Comprehensive metrics dashboard with trends and recommendations

**Phase 3 (12 weeks) - Professional Features:**
- **Semantic Knowledge Graph**: Visual knowledge management with fact relationships
- **Advanced Configuration**: System administration panel with user preferences
- **Data Management Tools**: Backup, restore, migration, and export functionality
- **Security & Access Controls**: User management and audit logging interface

#### **Backend Infrastructure Priorities**
- **Security Hardening**: Authentication, authorization, and rate limiting for API endpoints
- **Performance Optimization**: Caching, connection pooling, and query optimization
- **Monitoring & Observability**: Prometheus metrics, structured logging, and alerting
- **Documentation**: API documentation, architecture diagrams, and deployment guides
- **Testing**: Load testing, stress testing, and chaos engineering

### **Mid-Term Goals (Q4 2025 - Q1 2026)**
- **Advanced Executive Capabilities**: Strategic learning, goal optimization, and executive memory management
- **Multi-Modal Processing**: Voice, image, and video input processing with executive oversight
- **Advanced Planning**: Executive-driven chain-of-thought reasoning and complex task orchestration
- **Real-Time Feedback**: User feedback integration with executive adaptation and optimization
- **Distributed Executive Architecture**: Multi-node executive coordination and resource sharing
- **Executive Analytics**: Decision quality metrics, strategic effectiveness, and goal achievement tracking

### **Long-Term Vision (2026+)**
- **Autonomous Cognitive Management**: Self-managing executive functions with strategic learning
- **Multi-Agent Executive Networks**: Distributed executive decision-making and resource coordination
- **Multimodal Executive Presence**: AR/VR integration with executive state visualization and control
- **Emotional Executive Intelligence**: Executive functions with emotional awareness and empathetic decision-making
- **Continuous Executive Learning**: Adaptive strategies, improved decision-making, and strategic evolution

### **Research & Innovation**
- **Executive Neural Networks**: Advanced neural architectures for strategic thinking and planning
- **Quantum Executive Processing**: Quantum-inspired decision-making and strategic optimization
- **Neuromorphic Executive Computing**: Brain-inspired executive function acceleration
- **Explainable Executive AI**: Transparent strategic reasoning and decision explanations
- **Human-Executive Collaboration**: Seamless integration of human and AI executive functions

## Development Guidelines

### **Code Quality Standards**
- **Type Safety**: Full type hints with runtime validation using protocols and dataclasses
- **Error Handling**: Comprehensive exception hierarchy with graceful degradation
- **Documentation**: Detailed docstrings with examples, parameter descriptions, and return types
- **Testing**: Unit tests, integration tests, and performance benchmarks for all components
- **Modularity**: Loosely coupled components with clear interfaces and dependency injection

### **Performance Requirements**
- **Real-Time Operations**: Memory operations must complete within 100ms for interactive use
- **Concurrent Access**: Support for multiple simultaneous users with thread-safe operations
- **Resource Efficiency**: Optimal memory usage with connection pooling and caching
- **Scalability**: Horizontal scaling support with stateless design where possible

### **Security Best Practices**
- **Input Validation**: Comprehensive validation and sanitization of all inputs
- **Credential Management**: Secure storage and handling of API keys and secrets
- **Access Control**: Role-based access control and authentication for sensitive operations
- **Audit Logging**: Comprehensive logging of all operations for security monitoring

### **Cognitive Principles**
- **Biologically-Inspired**: Human cognitive science as the foundation for all algorithms
- **Explainable Intelligence**: All processes must be traceable and understandable
- **Adaptive Learning**: Systems should learn and improve from experience
- **Emotional Awareness**: Consider emotional context in all cognitive processes

## Testing Strategy

### **Test Coverage Requirements**
- **Unit Tests**: Individual component functionality with >90% code coverage
- **Integration Tests**: Cross-component communication and data flow validation
- **Performance Tests**: Memory operation speed, throughput, and resource usage
- **Cognitive Tests**: Human-likeness benchmarking and behavior validation
- **Security Tests**: Input validation, authentication, and authorization testing

### **Test Organization**
```
tests/
‚îú‚îÄ‚îÄ unit/                    # Unit tests for individual components
‚îÇ   ‚îú‚îÄ‚îÄ test_memory_system.py
‚îÇ   ‚îú‚îÄ‚îÄ test_stm_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_ltm_integration.py
‚îú‚îÄ‚îÄ integration/             # Integration tests
‚îÇ   ‚îú‚îÄ‚îÄ test_episodic_integration.py
‚îÇ   ‚îú‚îÄ‚îÄ test_semantic_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_prospective_integration.py
‚îú‚îÄ‚îÄ performance/             # Performance and load tests
‚îÇ   ‚îú‚îÄ‚îÄ test_memory_performance.py
‚îÇ   ‚îî‚îÄ‚îÄ test_concurrent_access.py
‚îú‚îÄ‚îÄ cognitive/               # Cognitive behavior tests
‚îÇ   ‚îú‚îÄ‚îÄ test_forgetting_curves.py
‚îÇ   ‚îî‚îÄ‚îÄ test_attention_modeling.py
‚îî‚îÄ‚îÄ security/               # Security and validation tests
    ‚îú‚îÄ‚îÄ test_input_validation.py
    ‚îî‚îÄ‚îÄ test_access_control.py
```

### **Continuous Integration**
- **Automated Testing**: All tests run on every commit and pull request
- **Code Quality**: Linting with ruff, type checking with mypy, and security scanning
- **Performance Monitoring**: Automated performance regression detection
- **Documentation**: Automatic API documentation generation and validation

## Recent Updates & Releases

### **Version 2.0.0 (July 2025) - Production-Grade Refactor**
- **Complete Memory System Overhaul**: Production-ready architecture with thread safety, error handling, and performance optimization
- **Unified Configuration Management**: Centralized configuration with validation and type safety
- **Advanced Prospective Memory**: ChromaDB-based persistent reminders with semantic search and automatic migration
- **Comprehensive Testing**: Full test coverage with integration, unit, and performance tests
- **Production Monitoring**: Health checks, metrics, and diagnostic reporting
- **Security Enhancements**: Input validation, sanitization, and secure credential management

### **Version 1.8.0 (June 2025) - Enhanced LTM & Semantic Memory**
- **Biologically-Inspired LTM**: Salience/recency weighting, memory decay, consolidation tracking
- **Semantic Memory System**: Structured fact storage with subject-predicate-object triples
- **Meta-Cognitive Feedback**: Self-monitoring and health diagnostics
- **Cross-System Linking**: Bidirectional associations and semantic clustering
- **Agent Fact Management**: Unified interface for structured knowledge storage

### **Version 1.7.0 (June 2025) - Unified Memory Interface**
- **Unified Memory API**: Consistent interface across all memory systems
- **Episodic Memory Enhancements**: Proactive recall, automatic summarization, and tagging
- **Procedural Memory Integration**: Skills and routines with STM/LTM storage
- **CLI Integration**: Comprehensive command-line interface for memory management
- **Streamlit Dashboard**: Modern web interface for system interaction

### **Version 1.6.0 (May 2025) - Neural Integration**
- **DPAD Networks**: Dual-Path Attention Dynamics for advanced attention modeling
- **LSHN Networks**: Latent Structured Hopfield Networks for associative memory
- **GPU Acceleration**: CUDA-optimized processing for embeddings and neural networks
- **Dream-State Processing**: Background consolidation with clustering and optimization

## Deployment & Production

#
### **Monitoring & Observability**
- **Health Checks**: `/health` endpoint with detailed system status
- **Metrics**: Prometheus-compatible metrics at `/metrics`
- **Logging**: Structured JSON logging with correlation IDs
- **Tracing**: OpenTelemetry integration for distributed tracing
- **Alerting**: Automated alerts for system health and performance issues



### **System Architecture**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Human-AI Cognition Framework                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  API Layer (FastAPI)                                           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ REST Endpoints (/memory, /agent, /prospective)           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Health Checks & Metrics                                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Authentication & Rate Limiting                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Cognitive Agent (Core)                                        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Memory System Coordinator                                ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Attention Mechanism                                      ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Meta-Cognitive Reflection                                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Neural Network Integration                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Memory Systems                                                ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ STM (Short-Term Memory)    ‚îú‚îÄ‚îÄ LTM (Long-Term Memory)    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Episodic Memory            ‚îú‚îÄ‚îÄ Semantic Memory           ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Prospective Memory         ‚îî‚îÄ‚îÄ Procedural Memory         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Neural Processing                                             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ DPAD Networks              ‚îú‚îÄ‚îÄ LSHN Networks             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Attention Dynamics         ‚îî‚îÄ‚îÄ Embedding Generation      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Storage & Persistence                                        ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ ChromaDB (Vector Storage)  ‚îú‚îÄ‚îÄ File System (Config)     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ GPU Acceleration (CUDA)    ‚îî‚îÄ‚îÄ Background Processing     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Memory Architecture**
```
Memory System Coordinator
‚îú‚îÄ‚îÄ Configuration Management (MemorySystemConfig)
‚îú‚îÄ‚îÄ Thread Safety (Locks, Pools)
‚îú‚îÄ‚îÄ Error Handling (Exception Hierarchy)
‚îú‚îÄ‚îÄ Performance Monitoring (Metrics, Health)
‚îî‚îÄ‚îÄ Memory Subsystems:
    ‚îú‚îÄ‚îÄ STM: In-memory with decay, vector search
    ‚îú‚îÄ‚îÄ LTM: Persistent ChromaDB, biologically-inspired
    ‚îú‚îÄ‚îÄ Episodic: Rich context, temporal indexing
    ‚îú‚îÄ‚îÄ Semantic: Structured facts, triple store
    ‚îú‚îÄ‚îÄ Prospective: Persistent reminders, migration
    ‚îî‚îÄ‚îÄ Procedural: Skills, action sequences
```

### **Data Flow**
```
Input ‚Üí Sensory Processing ‚Üí Attention Mechanism ‚Üí Memory System
                                ‚Üì
Context Retrieval ‚Üê Memory Search ‚Üê Consolidation Process
                                ‚Üì
LLM Processing ‚Üí Response Generation ‚Üí Memory Storage
                                ‚Üì
Background Processing ‚Üí Dream State ‚Üí Memory Optimization
```

## Contributing

### **Getting Started**
1. **Fork the Repository**: Create your own fork of the project
2. **Set Up Development Environment**: Follow the installation guide above
3. **Run Tests**: Ensure all tests pass before making changes
4. **Make Changes**: Follow the development guidelines and coding standards
5. **Submit Pull Request**: Include comprehensive tests and documentation


### **Code Review Process**
- All changes require peer review and approval
- Automated tests must pass before merging
- Documentation updates required for API changes
- Performance impact assessment for core changes

## Current System Status (July 2025)

### ‚úÖ **Complete & Production-Ready**
- **Executive Functioning**: Strategic planning, decision-making, task management, resource allocation
- **Short-Term Memory**: 7-item capacity, LRU eviction, activation decay, vector storage  
- **Long-Term Memory**: Persistent storage, forgetting curves, consolidation tracking
- **Episodic Memory**: Event-based memories with timeline and narrative construction
- **Semantic Memory**: Structured knowledge with concept relationships and inference
- **Procedural Memory**: Skills and action sequences with usage optimization
- **Prospective Memory**: Future-oriented tasks and reminders with scheduling
- **Attention System**: Fatigue modeling, neural enhancement, cognitive load tracking
- **Neural Networks**: DPAD attention dynamics and LSHN associative memory
- **Dream State**: Background consolidation with neural integration
- **Meta-Cognition**: Self-reflection, performance monitoring, health diagnostics
- **Production Features**: Thread safety, error handling, monitoring, resource management

### üéØ **System Completeness**: ~95%
The Human-AI Cognition Framework now represents a **nearly complete** biologically-inspired cognitive architecture with:

- **30+ Production-Ready Components**: All core cognitive functions implemented
- **8 Integrated Memory Systems**: Complete memory hierarchy from sensory to long-term
- **Executive Control**: Full prefrontal cortex simulation with strategic thinking
- **Neural Enhancement**: Advanced attention and associative memory networks  
- **Real-Time Processing**: Sub-100ms memory operations for interactive use
- **Comprehensive Testing**: 100% pass rate across all integration tests
- **Production Deployment**: Thread-safe, scalable, monitored, and documented

The framework now provides **human-like cognitive capabilities** including strategic planning, emotional memory, attention management, and self-reflection - making it one of the most complete biologically-inspired AI architectures available.

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- **OpenAI**: For providing the GPT-4 language model
- **ChromaDB**: For the vector database infrastructure
- **Hugging Face**: For the sentence-transformers library
- **PyTorch**: For neural network components
- **FastAPI**: For the modern web framework
- **The Open Source Community**: For countless libraries and tools

## Support

- **Documentation**: Comprehensive guides in the `/docs` directory
- **Issues**: Report bugs and request features via GitHub Issues
- **Discussions**: Join the community discussions for support and collaboration
- **Email**: Contact the development team at [contact@human-ai-cognition.org]

---

**Human-AI Cognition Framework** - Building the future of human-like artificial intelligence through biologically-inspired cognitive architectures.

*Version 2.0.0 (July 2025) - Production-Grade Cognitive AI*

## Chat Interface Status (Fully Implemented)
The production chat interface now provides deterministic, explainable context assembly with full provenance, adaptive metacognitive regulation, dynamic retrieval & consolidation heuristics, and comprehensive latency + selectivity metrics. Legacy planning checklist removed (all core items delivered; performance tuning incorporated into adaptive mechanisms and test suite).
- stm_hits / ltm_hits
