"""
Episodic Memory System for Human-AI Cognition Framework

Implements biologically-inspired episodic memory with rich temporal and contextual metadata.
Uses ChromaDB for semantic storage with cross-references to STM and LTM systems.

Key Features:
- Rich metadata (timestamp, context, emotional valence, associated_stm_ids, summary)
- Cross-references to STM and LTM memories  
- Temporal clustering and autobiographical organization
- Integration with memory consolidation pipeline
- Semantic search and temporal retrieval patterns
"""

from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
import json
import logging
from pathlib import Path
import hashlib
import uuid

try:
    import chromadb
    from chromadb.config import Settings
    CHROMADB_AVAILABLE = True
except ImportError:
    CHROMADB_AVAILABLE = False
    chromadb = None

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    SentenceTransformer = None

logger = logging.getLogger(__name__)

@dataclass
class EpisodicContext:
    """Rich contextual information for episodic memories"""
    location: Optional[str] = None
    emotional_state: float = 0.0  # -1.0 to 1.0
    cognitive_load: float = 0.0   # 0.0 to 1.0
    attention_focus: List[str] = field(default_factory=list)
    interaction_type: str = "conversation"  # conversation, reflection, consolidation, etc.
    participants: List[str] = field(default_factory=list)
    environmental_factors: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            "location": self.location,
            "emotional_state": self.emotional_state,
            "cognitive_load": self.cognitive_load,
            "attention_focus": self.attention_focus,
            "interaction_type": self.interaction_type,
            "participants": self.participants,
            "environmental_factors": self.environmental_factors
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "EpisodicContext":
        """Create from dictionary"""
        return cls(
            location=data.get("location"),
            emotional_state=data.get("emotional_state", 0.0),
            cognitive_load=data.get("cognitive_load", 0.0),
            attention_focus=data.get("attention_focus", []),
            interaction_type=data.get("interaction_type", "conversation"),
            participants=data.get("participants", []),
            environmental_factors=data.get("environmental_factors", {})
        )

@dataclass 
class EpisodicMemory:
    """Individual episodic memory record with rich metadata"""
    id: str
    summary: str  # Brief summary of the episode
    detailed_content: str  # Full content/narrative of the episode
    timestamp: datetime
    duration: timedelta = field(default_factory=lambda: timedelta(minutes=1))
    
    # Contextual information
    context: EpisodicContext = field(default_factory=EpisodicContext)
    
    # Memory system cross-references
    associated_stm_ids: List[str] = field(default_factory=list)
    associated_ltm_ids: List[str] = field(default_factory=list)
    source_memory_ids: List[str] = field(default_factory=list)  # Memories that contributed to this episode
    
    # Episodic characteristics
    importance: float = 0.5  # 0.0 to 1.0
    emotional_valence: float = 0.0  # -1.0 to 1.0 
    vividness: float = 0.5  # How clear/detailed the memory is (0.0 to 1.0)
    confidence: float = 0.8  # Confidence in memory accuracy (0.0 to 1.0)
    
    # Autobiographical organization
    life_period: Optional[str] = None  # e.g., "work_conversation", "learning_session"
    episode_sequence: int = 0  # Sequence number within a period
    related_episodes: List[str] = field(default_factory=list)  # Related episode IDs
    
    # Access and consolidation tracking
    access_count: int = 0
    last_access: datetime = field(default_factory=datetime.now)
    consolidation_strength: float = 0.0  # How well consolidated this memory is
    rehearsal_count: int = 0  # How many times it's been rehearsed/recalled
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage"""
        return {
            "id": self.id,
            "summary": self.summary,
            "detailed_content": self.detailed_content,
            "timestamp": self.timestamp.isoformat(),
            "duration": self.duration.total_seconds(),
            "context": self.context.to_dict(),
            "associated_stm_ids": self.associated_stm_ids,
            "associated_ltm_ids": self.associated_ltm_ids,
            "source_memory_ids": self.source_memory_ids,
            "importance": self.importance,
            "emotional_valence": self.emotional_valence,
            "vividness": self.vividness,
            "confidence": self.confidence,
            "life_period": self.life_period,
            "episode_sequence": self.episode_sequence,
            "related_episodes": self.related_episodes,
            "access_count": self.access_count,
            "last_access": self.last_access.isoformat(),
            "consolidation_strength": self.consolidation_strength,
            "rehearsal_count": self.rehearsal_count
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "EpisodicMemory":
        """Create from dictionary"""
        return cls(
            id=data["id"],
            summary=data["summary"],
            detailed_content=data["detailed_content"],
            timestamp=datetime.fromisoformat(data["timestamp"]),
            duration=timedelta(seconds=data.get("duration", 60)),
            context=EpisodicContext.from_dict(data.get("context", {})),
            associated_stm_ids=data.get("associated_stm_ids", []),
            associated_ltm_ids=data.get("associated_ltm_ids", []),
            source_memory_ids=data.get("source_memory_ids", []),
            importance=data.get("importance", 0.5),
            emotional_valence=data.get("emotional_valence", 0.0),
            vividness=data.get("vividness", 0.5),
            confidence=data.get("confidence", 0.8),
            life_period=data.get("life_period"),
            episode_sequence=data.get("episode_sequence", 0),
            related_episodes=data.get("related_episodes", []),
            access_count=data.get("access_count", 0),
            last_access=datetime.fromisoformat(data.get("last_access", datetime.now().isoformat())),
            consolidation_strength=data.get("consolidation_strength", 0.0),
            rehearsal_count=data.get("rehearsal_count", 0)
        )

@dataclass
class EpisodicSearchResult:
    """Result from episodic memory search"""
    memory: EpisodicMemory
    relevance: float  # 0.0 to 1.0
    match_type: str  # "semantic", "temporal", "contextual", "cross_reference"
    search_metadata: Dict[str, Any] = field(default_factory=dict)

class EpisodicMemorySystem:
    """
    Episodic Memory System for autobiographical memory storage and retrieval
    
    Features:
    - Rich contextual metadata storage
    - Cross-references to STM and LTM systems
    - Temporal and semantic search capabilities
    - Autobiographical organization and clustering
    - Integration with memory consolidation pipeline
    """
    
    def __init__(
        self,
        chroma_persist_dir: Optional[str] = None,
        collection_name: str = "episodic_memories",
        embedding_model: str = "all-MiniLM-L6-v2",
        enable_json_backup: bool = True,
        storage_path: Optional[str] = None
    ):
        """
        Initialize Episodic Memory System
        
        Args:
            chroma_persist_dir: ChromaDB persistence directory
            collection_name: Name of ChromaDB collection
            embedding_model: SentenceTransformer model name
            enable_json_backup: Whether to maintain JSON backups
            storage_path: Path for JSON storage
        """
        # Storage configuration
        self.chroma_persist_dir = Path(chroma_persist_dir or "data/memory_stores/chroma_episodic")
        self.storage_path = Path(storage_path or "data/memory_stores/episodic")
        self.collection_name = collection_name
        self.enable_json_backup = enable_json_backup
        
        # Create directories
        self.chroma_persist_dir.mkdir(parents=True, exist_ok=True)
        if self.enable_json_backup:
            self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # In-memory index for fast access
        self.episodes: Dict[str, EpisodicMemory] = {}
        self.temporal_index: Dict[str, List[str]] = {}  # date -> episode_ids
        self.period_index: Dict[str, List[str]] = {}    # life_period -> episode_ids
        
        # Vector database setup
        self.use_vector_db = CHROMADB_AVAILABLE
        self.chroma_client = None
        self.collection = None
        
        # Embedding model
        self.embedding_model = None
        if self.use_vector_db and SENTENCE_TRANSFORMERS_AVAILABLE:
            try:
                self.embedding_model = SentenceTransformer(embedding_model)
                logger.info(f"Episodic Memory loaded embedding model: {embedding_model}")
            except Exception as e:
                logger.warning(f"Failed to load embedding model {embedding_model}: {e}")
                self.embedding_model = None
        
        # Initialize ChromaDB
        if self.use_vector_db:
            self._initialize_chromadb()
        
        # Load existing memories
        self._load_memories()
        
        # Statistics
        self.stats = {
            "episodes_created": 0,
            "total_rehearsals": 0,
            "consolidation_events": 0,
            "cross_references_created": 0
        }
        
        logger.info(f"Episodic Memory System initialized with {len(self.episodes)} episodes")
        logger.info(f"Vector DB: {self.use_vector_db}, JSON backup: {self.enable_json_backup}")
    
    def _initialize_chromadb(self):
        """Initialize ChromaDB client and collection"""
        if not CHROMADB_AVAILABLE:
            logger.warning("ChromaDB not available, episodic memory will use JSON only")
            self.use_vector_db = False
            return
        
        try:
            # Initialize client with persistence
            self.chroma_client = chromadb.PersistentClient(
                path=str(self.chroma_persist_dir),
                settings=Settings(
                    allow_reset=True,
                    anonymized_telemetry=False
                )
            )
            
            # Get or create collection
            try:
                self.collection = self.chroma_client.get_collection(name=self.collection_name)
                logger.info(f"Connected to existing episodic memory collection: {self.collection_name}")
            except Exception:
                # Create new collection
                self.collection = self.chroma_client.create_collection(
                    name=self.collection_name,
                    metadata={"description": "Episodic memory storage with rich contextual metadata"}
                )
                logger.info(f"Created new episodic memory collection: {self.collection_name}")
        
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB for episodic memory: {e}")
            self.use_vector_db = False
            self.chroma_client = None
            self.collection = None
    
    def _generate_embedding(self, text: str) -> Optional[List[float]]:
        """Generate embedding for text content"""
        if not self.embedding_model:
            return None
        
        try:
            embedding = self.embedding_model.encode(text)
            return embedding.tolist()
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            return None
    
    def _content_to_text(self, episode: EpisodicMemory) -> str:
        """Convert episode to searchable text"""
        # Combine summary and content for comprehensive search
        text_parts = [
            episode.summary,
            episode.detailed_content,
            f"Context: {episode.context.interaction_type}",
            f"Participants: {', '.join(episode.context.participants)}",
            f"Location: {episode.context.location or 'unknown'}",
            f"Period: {episode.life_period or 'general'}"
        ]
        
        return " | ".join(filter(None, text_parts))
    
    def create_episode(
        self,
        summary: str,
        detailed_content: str,
        context: Optional[EpisodicContext] = None,
        associated_stm_ids: Optional[List[str]] = None,
        associated_ltm_ids: Optional[List[str]] = None,
        source_memory_ids: Optional[List[str]] = None,
        importance: float = 0.5,
        emotional_valence: float = 0.0,
        life_period: Optional[str] = None
    ) -> str:
        """
        Create a new episodic memory
        
        Args:
            summary: Brief summary of the episode
            detailed_content: Full content/narrative
            context: Rich contextual information
            associated_stm_ids: Related STM memory IDs
            associated_ltm_ids: Related LTM memory IDs
            source_memory_ids: Source memories that contributed to this episode
            importance: Importance score (0.0 to 1.0)
            emotional_valence: Emotional valence (-1.0 to 1.0)
            life_period: Life period categorization
        
        Returns:
            Episode ID
        """
        episode_id = f"ep_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        # Create episode
        episode = EpisodicMemory(
            id=episode_id,
            summary=summary,
            detailed_content=detailed_content,
            timestamp=datetime.now(),
            context=context or EpisodicContext(),
            associated_stm_ids=associated_stm_ids or [],
            associated_ltm_ids=associated_ltm_ids or [],
            source_memory_ids=source_memory_ids or [],
            importance=importance,
            emotional_valence=emotional_valence,
            life_period=life_period
        )
        
        # Determine episode sequence within period
        if life_period:
            if life_period not in self.period_index:
                self.period_index[life_period] = []
            episode.episode_sequence = len(self.period_index[life_period])
        
        # Store episode
        self.episodes[episode_id] = episode
        
        # Update indices
        self._update_indices(episode)
        
        # Store in ChromaDB
        if self.use_vector_db and self.collection:
            self._store_in_chromadb(episode)
        
        # Save JSON backup
        if self.enable_json_backup:
            self._save_episode_json(episode)
        
        # Update statistics
        self.stats["episodes_created"] += 1
        if associated_stm_ids or associated_ltm_ids:
            self.stats["cross_references_created"] += len(associated_stm_ids or []) + len(associated_ltm_ids or [])
        
        logger.info(f"Created episodic memory: {episode_id} (summary: '{summary[:50]}...')")
        return episode_id
    
    def _update_indices(self, episode: EpisodicMemory):
        """Update search indices for an episode"""
        # Temporal index
        date_key = episode.timestamp.strftime("%Y-%m-%d")
        if date_key not in self.temporal_index:
            self.temporal_index[date_key] = []
        if episode.id not in self.temporal_index[date_key]:
            self.temporal_index[date_key].append(episode.id)
        
        # Period index
        if episode.life_period:
            if episode.life_period not in self.period_index:
                self.period_index[episode.life_period] = []
            if episode.id not in self.period_index[episode.life_period]:
                self.period_index[episode.life_period].append(episode.id)
    
    def _store_in_chromadb(self, episode: EpisodicMemory):
        """Store episode in ChromaDB collection"""
        if not self.collection:
            return
        
        try:
            text_content = self._content_to_text(episode)
            embedding = self._generate_embedding(text_content)
            
            # Prepare metadata for ChromaDB
            metadata = {
                "summary": episode.summary,
                "timestamp": episode.timestamp.isoformat(),
                "duration": episode.duration.total_seconds(),
                "importance": episode.importance,
                "emotional_valence": episode.emotional_valence,
                "vividness": episode.vividness,
                "confidence": episode.confidence,
                "life_period": episode.life_period or "",
                "episode_sequence": episode.episode_sequence,
                "interaction_type": episode.context.interaction_type,
                "cognitive_load": episode.context.cognitive_load,
                "emotional_state": episode.context.emotional_state,
                "participants": ",".join(episode.context.participants),
                "location": episode.context.location or "",
                "associated_stm_ids": ",".join(episode.associated_stm_ids),
                "associated_ltm_ids": ",".join(episode.associated_ltm_ids),
                "source_memory_ids": ",".join(episode.source_memory_ids),
                "access_count": episode.access_count,
                "consolidation_strength": episode.consolidation_strength,
                "rehearsal_count": episode.rehearsal_count
            }
            
            # Store in ChromaDB
            try:
                # Check if exists and update
                existing = self.collection.get(ids=[episode.id])
                if existing['ids']:
                    self.collection.update(
                        ids=[episode.id],
                        documents=[text_content],
                        metadatas=[metadata],
                        embeddings=[embedding] if embedding else None
                    )
                else:
                    self.collection.add(
                        ids=[episode.id],
                        documents=[text_content],
                        metadatas=[metadata],
                        embeddings=[embedding] if embedding else None
                    )
            except Exception:
                # Fallback to add
                self.collection.add(
                    ids=[episode.id],
                    documents=[text_content],
                    metadatas=[metadata], 
                    embeddings=[embedding] if embedding else None
                )
                
        except Exception as e:
            logger.error(f"Failed to store episode {episode.id} in ChromaDB: {e}")
    
    def _save_episode_json(self, episode: EpisodicMemory):
        """Save episode to JSON file"""
        if not self.enable_json_backup:
            return
        
        episode_file = self.storage_path / f"{episode.id}.json"
        try:
            with open(episode_file, 'w', encoding='utf-8') as f:
                json.dump(episode.to_dict(), f, indent=2, default=str)
        except Exception as e:
            logger.error(f"Failed to save episode {episode.id} to JSON: {e}")
    
    def _load_memories(self):
        """Load memories from both JSON and ChromaDB"""
        # Load from JSON files first
        if self.enable_json_backup:
            self._load_from_json()
        
        # Load from ChromaDB and sync
        if self.use_vector_db and self.collection:
            self._load_from_chromadb()
    
    def _load_from_json(self):
        """Load episodes from JSON files"""
        if not self.storage_path.exists():
            return
        
        for json_file in self.storage_path.glob("ep_*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    episode = EpisodicMemory.from_dict(data)
                    self.episodes[episode.id] = episode
                    self._update_indices(episode)
            except Exception as e:
                logger.error(f"Failed to load episode from {json_file}: {e}")
        
        if self.episodes:
            logger.info(f"Loaded {len(self.episodes)} episodes from JSON files")
    
    def _load_from_chromadb(self):
        """Load episodes from ChromaDB and sync with in-memory store"""
        if not self.collection:
            return
        
        try:
            # Get all episodes from ChromaDB
            all_results = self.collection.get()
            
            if all_results['ids']:
                for i, episode_id in enumerate(all_results['ids']):
                    if episode_id not in self.episodes:
                        # Reconstruct episode from ChromaDB
                        metadata = all_results['metadatas'][i] if all_results['metadatas'] else {}
                        document = all_results['documents'][i] if all_results['documents'] else ""
                        
                        try:
                            # Extract content from document (contains our formatted text)
                            content_parts = document.split(" | ")
                            summary = content_parts[0] if content_parts else "Recovered episode"
                            detailed_content = content_parts[1] if len(content_parts) > 1 else document
                            
                            # Reconstruct context
                            context = EpisodicContext(
                                interaction_type=metadata.get('interaction_type', 'conversation'),
                                cognitive_load=metadata.get('cognitive_load', 0.0),
                                emotional_state=metadata.get('emotional_state', 0.0),
                                participants=metadata.get('participants', '').split(',') if metadata.get('participants') else [],
                                location=metadata.get('location') if metadata.get('location') else None
                            )
                            
                            # Create episode
                            episode = EpisodicMemory(
                                id=episode_id,
                                summary=metadata.get('summary', summary),
                                detailed_content=detailed_content,
                                timestamp=datetime.fromisoformat(metadata.get('timestamp', datetime.now().isoformat())),
                                duration=timedelta(seconds=metadata.get('duration', 60)),
                                context=context,
                                associated_stm_ids=metadata.get('associated_stm_ids', '').split(',') if metadata.get('associated_stm_ids') else [],
                                associated_ltm_ids=metadata.get('associated_ltm_ids', '').split(',') if metadata.get('associated_ltm_ids') else [],
                                source_memory_ids=metadata.get('source_memory_ids', '').split(',') if metadata.get('source_memory_ids') else [],
                                importance=metadata.get('importance', 0.5),
                                emotional_valence=metadata.get('emotional_valence', 0.0),
                                vividness=metadata.get('vividness', 0.5),
                                confidence=metadata.get('confidence', 0.8),
                                life_period=metadata.get('life_period') if metadata.get('life_period') else None,
                                episode_sequence=metadata.get('episode_sequence', 0),
                                access_count=metadata.get('access_count', 0),
                                consolidation_strength=metadata.get('consolidation_strength', 0.0),
                                rehearsal_count=metadata.get('rehearsal_count', 0)
                            )
                            
                            self.episodes[episode_id] = episode
                            self._update_indices(episode)
                            
                        except Exception as e:
                            logger.error(f"Failed to reconstruct episode {episode_id} from ChromaDB: {e}")
                
                logger.info(f"Synchronized {len(all_results['ids'])} episodes from ChromaDB")
        
        except Exception as e:
            logger.error(f"Failed to load from ChromaDB: {e}")
    
    def search_semantic(
        self,
        query: str,
        max_results: int = 10,
        min_similarity: float = 0.6,
        date_range: Optional[Tuple[datetime, datetime]] = None,
        life_periods: Optional[List[str]] = None,
        min_importance: float = 0.0
    ) -> List[EpisodicSearchResult]:
        """
        Search episodes using semantic similarity
        
        Args:
            query: Search query
            max_results: Maximum number of results
            min_similarity: Minimum similarity threshold
            date_range: Optional date range filter (start, end)
            life_periods: Optional life period filter
            min_importance: Minimum importance threshold
        
        Returns:
            List of search results
        """
        if not self.use_vector_db or not self.collection:
            return self._fallback_search(query, max_results, date_range, life_periods, min_importance)
        
        try:
            # Build where clause for filtering
            where_clause = {}
            
            if date_range:
                where_clause["timestamp"] = {
                    "$gte": date_range[0].isoformat(),
                    "$lte": date_range[1].isoformat()
                }
            
            if life_periods:
                where_clause["life_period"] = {"$in": life_periods}
            
            if min_importance > 0.0:
                where_clause["importance"] = {"$gte": min_importance}
            
            # Perform semantic search
            results = self.collection.query(
                query_texts=[query],
                n_results=max_results,
                where=where_clause if where_clause else None
            )
            
            search_results = []
            if results['ids'] and results['ids'][0]:
                for i, episode_id in enumerate(results['ids'][0]):
                    distance = results['distances'][0][i] if results['distances'] else 0.0
                    similarity = 1.0 - distance  # Convert distance to similarity
                    
                    if similarity >= min_similarity and episode_id in self.episodes:
                        result = EpisodicSearchResult(
                            memory=self.episodes[episode_id],
                            relevance=similarity,
                            match_type="semantic",
                            search_metadata={"query": query, "distance": distance}
                        )
                        search_results.append(result)
            
            return search_results
        
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return self._fallback_search(query, max_results, date_range, life_periods, min_importance)
    
    def _fallback_search(
        self,
        query: str,
        max_results: int,
        date_range: Optional[Tuple[datetime, datetime]],
        life_periods: Optional[List[str]],
        min_importance: float
    ) -> List[EpisodicSearchResult]:
        """Fallback search using content matching"""
        results = []
        query_lower = query.lower()
        
        for episode in self.episodes.values():
            # Apply filters
            if date_range and not (date_range[0] <= episode.timestamp <= date_range[1]):
                continue
            
            if life_periods and episode.life_period not in life_periods:
                continue
            
            if episode.importance < min_importance:
                continue
            
            # Calculate relevance based on text matching
            text_content = self._content_to_text(episode).lower()
            relevance = 0.0
            
            # Simple keyword matching for fallback
            query_words = query_lower.split()
            matched_words = sum(1 for word in query_words if word in text_content)
            if query_words:
                relevance = matched_words / len(query_words)
            
            if relevance > 0.0:
                result = EpisodicSearchResult(
                    memory=episode,
                    relevance=relevance,
                    match_type="content",
                    search_metadata={"query": query, "matched_words": matched_words}
                )
                results.append(result)
        
        # Sort by relevance and limit results
        results.sort(key=lambda x: x.relevance, reverse=True)
        return results[:max_results]
    
    def get_episode(self, episode_id: str) -> Optional[EpisodicMemory]:
        """Retrieve episode by ID"""
        episode = self.episodes.get(episode_id)
        if episode:
            # Update access tracking
            episode.access_count += 1
            episode.last_access = datetime.now()
            
            # Update in storage
            if self.use_vector_db and self.collection:
                self._store_in_chromadb(episode)
            if self.enable_json_backup:
                self._save_episode_json(episode)
        
        return episode
    
    def get_episodes_by_date(
        self,
        date: datetime,
        date_range_days: int = 0
    ) -> List[EpisodicMemory]:
        """Get episodes by date or date range"""
        if date_range_days == 0:
            # Single day
            date_key = date.strftime("%Y-%m-%d")
            episode_ids = self.temporal_index.get(date_key, [])
        else:
            # Date range
            episode_ids = []
            for i in range(date_range_days + 1):
                check_date = date + timedelta(days=i)
                date_key = check_date.strftime("%Y-%m-%d")
                episode_ids.extend(self.temporal_index.get(date_key, []))
        
        return [self.episodes[eid] for eid in episode_ids if eid in self.episodes]
    
    def get_episodes_by_period(self, life_period: str) -> List[EpisodicMemory]:
        """Get episodes by life period"""
        episode_ids = self.period_index.get(life_period, [])
        episodes = [self.episodes[eid] for eid in episode_ids if eid in self.episodes]
        return sorted(episodes, key=lambda e: e.episode_sequence)
    
    def find_related_episodes(
        self,
        episode_id: str,
        max_results: int = 5,
        similarity_threshold: float = 0.7
    ) -> List[EpisodicSearchResult]:
        """Find episodes related to the given episode"""
        episode = self.episodes.get(episode_id)
        if not episode:
            return []
        
        # Use the episode content as search query
        query = f"{episode.summary} {episode.detailed_content}"
        
        # Search for similar episodes
        results = self.search_semantic(
            query=query,
            max_results=max_results + 1,  # +1 because we'll exclude the original
            min_similarity=similarity_threshold
        )
        
        # Filter out the original episode
        related_results = [r for r in results if r.memory.id != episode_id]
        return related_results[:max_results]
    
    def get_cross_referenced_memories(
        self,
        episode_id: str
    ) -> Dict[str, List[str]]:
        """Get all cross-referenced memories for an episode"""
        episode = self.episodes.get(episode_id)
        if not episode:
            return {"stm": [], "ltm": [], "source": []}
        
        return {
            "stm": episode.associated_stm_ids,
            "ltm": episode.associated_ltm_ids,
            "source": episode.source_memory_ids
        }
    
    def add_cross_reference(
        self,
        episode_id: str,
        memory_id: str,
        memory_type: str  # "stm", "ltm", or "source"
    ) -> bool:
        """Add a cross-reference to an episode"""
        episode = self.episodes.get(episode_id)
        if not episode:
            return False
        
        # Add reference if not already present
        if memory_type == "stm" and memory_id not in episode.associated_stm_ids:
            episode.associated_stm_ids.append(memory_id)
        elif memory_type == "ltm" and memory_id not in episode.associated_ltm_ids:
            episode.associated_ltm_ids.append(memory_id)
        elif memory_type == "source" and memory_id not in episode.source_memory_ids:
            episode.source_memory_ids.append(memory_id)
        else:
            return False  # Already exists or invalid type
        
        # Update storage
        if self.use_vector_db and self.collection:
            self._store_in_chromadb(episode)
        if self.enable_json_backup:
            self._save_episode_json(episode)
        
        self.stats["cross_references_created"] += 1
        return True
    
    def rehearse_episode(self, episode_id: str) -> bool:
        """Mark an episode as rehearsed/recalled"""
        episode = self.episodes.get(episode_id)
        if not episode:
            return False
        
        episode.rehearsal_count += 1
        episode.access_count += 1
        episode.last_access = datetime.now()
        
        # Rehearsal strengthens memory
        episode.consolidation_strength = min(1.0, episode.consolidation_strength + 0.1)
        episode.vividness = min(1.0, episode.vividness + 0.05)
        
        # Update storage
        if self.use_vector_db and self.collection:
            self._store_in_chromadb(episode)
        if self.enable_json_backup:
            self._save_episode_json(episode)
        
        self.stats["total_rehearsals"] += 1
        return True
    
    def consolidate_episode(
        self,
        episode_id: str,
        strength_boost: float = 0.2
    ) -> bool:
        """Strengthen an episode through consolidation"""
        episode = self.episodes.get(episode_id)
        if not episode:
            return False
        
        episode.consolidation_strength = min(1.0, episode.consolidation_strength + strength_boost)
        episode.confidence = min(1.0, episode.confidence + 0.1)
        
        # Update storage
        if self.use_vector_db and self.collection:
            self._store_in_chromadb(episode)
        if self.enable_json_backup:
            self._save_episode_json(episode)
        
        self.stats["consolidation_events"] += 1
        return True
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get episodic memory system statistics"""
        current_stats = self.stats.copy()
        current_stats.update({
            "total_episodes": len(self.episodes),
            "life_periods": len(self.period_index),
            "date_range": self._get_date_range(),
            "avg_importance": self._get_average_importance(),
            "avg_consolidation": self._get_average_consolidation(),
            "memory_distribution": self._get_memory_distribution()
        })
        return current_stats
    
    def _get_date_range(self) -> Dict[str, Optional[str]]:
        """Get earliest and latest episode dates"""
        if not self.episodes:
            return {"earliest": None, "latest": None}
        
        timestamps = [e.timestamp for e in self.episodes.values()]
        return {
            "earliest": min(timestamps).isoformat(),
            "latest": max(timestamps).isoformat()
        }
    
    def _get_average_importance(self) -> float:
        """Get average importance across all episodes"""
        if not self.episodes:
            return 0.0
        return sum(e.importance for e in self.episodes.values()) / len(self.episodes)
    
    def _get_average_consolidation(self) -> float:
        """Get average consolidation strength across all episodes"""
        if not self.episodes:
            return 0.0
        return sum(e.consolidation_strength for e in self.episodes.values()) / len(self.episodes)
    
    def _get_memory_distribution(self) -> Dict[str, int]:
        """Get distribution of episodes by life period"""
        distribution = {}
        for episode in self.episodes.values():
            period = episode.life_period or "general"
            distribution[period] = distribution.get(period, 0) + 1
        return distribution
    
    def cleanup_old_episodes(
        self,
        days_threshold: int = 90,
        min_importance: float = 0.3,
        min_consolidation: float = 0.2
    ) -> int:
        """Clean up old episodes with low importance and consolidation"""
        cutoff_date = datetime.now() - timedelta(days=days_threshold)
        removed_count = 0
        
        episodes_to_remove = []
        for episode in self.episodes.values():
            if (episode.timestamp < cutoff_date and 
                episode.importance < min_importance and 
                episode.consolidation_strength < min_consolidation):
                episodes_to_remove.append(episode.id)
        
        for episode_id in episodes_to_remove:
            if self._remove_episode(episode_id):
                removed_count += 1
        
        if removed_count > 0:
            logger.info(f"Cleaned up {removed_count} old episodes")
        
        return removed_count
    
    def _remove_episode(self, episode_id: str) -> bool:
        """Remove an episode from all storage"""
        if episode_id not in self.episodes:
            return False
        
        episode = self.episodes[episode_id]
        
        # Remove from ChromaDB
        if self.use_vector_db and self.collection:
            try:
                self.collection.delete(ids=[episode_id])
            except Exception as e:
                logger.error(f"Failed to remove episode {episode_id} from ChromaDB: {e}")
        
        # Remove JSON file
        if self.enable_json_backup:
            json_file = self.storage_path / f"{episode_id}.json"
            if json_file.exists():
                try:
                    json_file.unlink()
                except Exception as e:
                    logger.error(f"Failed to remove JSON file for {episode_id}: {e}")
        
        # Update indices
        date_key = episode.timestamp.strftime("%Y-%m-%d")
        if date_key in self.temporal_index:
            self.temporal_index[date_key] = [eid for eid in self.temporal_index[date_key] if eid != episode_id]
        
        if episode.life_period and episode.life_period in self.period_index:
            self.period_index[episode.life_period] = [eid for eid in self.period_index[episode.life_period] if eid != episode_id]
        
        # Remove from memory
        del self.episodes[episode_id]
        
        return True
    
    def shutdown(self):
        """Shutdown episodic memory system"""
        logger.info("Shutting down Episodic Memory System...")
        
        # Save all episodes
        if self.enable_json_backup:
            for episode in self.episodes.values():
                self._save_episode_json(episode)
        
        # ChromaDB persistence is automatic
        logger.info(f"Episodic Memory System shutdown complete. {len(self.episodes)} episodes preserved.")
    
    def find_episodes_by_memory_reference(
        self,
        memory_id: str,
        memory_type: str = "any"  # "stm", "ltm", "source", or "any"
    ) -> List[EpisodicMemory]:
        """
        Find episodes that reference a specific memory ID
        
        Args:
            memory_id: The memory ID to search for
            memory_type: Type of memory reference to search ("stm", "ltm", "source", "any")
        
        Returns:
            List of episodes that reference the memory
        """
        found_episodes = []
        
        for episode in self.episodes.values():
            # Check appropriate reference lists based on memory_type
            if memory_type in ("stm", "any") and memory_id in episode.associated_stm_ids:
                found_episodes.append(episode)
            elif memory_type in ("ltm", "any") and memory_id in episode.associated_ltm_ids:
                found_episodes.append(episode)
            elif memory_type in ("source", "any") and memory_id in episode.source_memory_ids:
                found_episodes.append(episode)
        
        # Sort by timestamp (most recent first)
        found_episodes.sort(key=lambda ep: ep.timestamp, reverse=True)
        return found_episodes
