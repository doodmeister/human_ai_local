#!/usr/bin/env python3
"""
Comprehensive test script for enhanced LTM features:
- Salience/Recency weighting in retrieval
- Decay/Forgetting functionality
- Consolidation tracking
- Meta-cognitive feedback
- Emotionally weighted consolidation
- Cross-system query/linking
"""
import sys
import os
sys.path.insert(0, os.path.abspath('.'))

from datetime import datetime, timedelta
from src.memory.ltm.long_term_memory import LongTermMemory
from src.memory.stm.short_term_memory import MemoryItem
import uuid

def test_enhanced_ltm_features():
    """Test all enhanced LTM features"""
    print("üß† Testing Enhanced Long-Term Memory Features")
    print("=" * 50)
    
    # Initialize LTM
    ltm = LongTermMemory(storage_path="data/memory_stores/ltm_enhanced_test")
    ltm.memories.clear()  # Clean state
    
    # Test 1: Salience/Recency Weighting in Retrieval
    print("\n1. üìä Testing Salience/Recency Weighting")
    
    # Store memories with different access patterns
    old_memory_id = str(uuid.uuid4())
    recent_memory_id = str(uuid.uuid4())
    frequent_memory_id = str(uuid.uuid4())
    
    # Old memory (low salience)
    ltm.store(
        memory_id=old_memory_id,
        content="old programming concept that's rarely used",
        memory_type="semantic",
        importance=0.5
    )
    ltm.memories[old_memory_id].last_access = datetime.now() - timedelta(days=30)
    
    # Recent memory (high recency)
    ltm.store(
        memory_id=recent_memory_id,
        content="recent programming concept learned yesterday",
        memory_type="semantic", 
        importance=0.5
    )
    ltm.memories[recent_memory_id].last_access = datetime.now() - timedelta(hours=1)
    
    # Frequently accessed memory (high salience)
    ltm.store(
        memory_id=frequent_memory_id,
        content="frequently used programming concept",
        memory_type="semantic",
        importance=0.5
    )
    ltm.memories[frequent_memory_id].access_count = 15
    ltm.memories[frequent_memory_id].last_access = datetime.now() - timedelta(days=7)
    
    # Search and check ordering (recent and frequent should rank higher)
    results = ltm.search_by_content("programming concept", max_results=10)
    print(f"   - Found {len(results)} results")
    for i, (record, score) in enumerate(results):
        print(f"   - Rank {i+1}: {record.content[:40]}... (score: {score:.3f})")
    
    # Test 2: Decay/Forgetting Functionality
    print("\n2. üï∞Ô∏è  Testing Decay/Forgetting")
    
    # Create memory that should decay
    decay_memory_id = str(uuid.uuid4())
    ltm.store(
        memory_id=decay_memory_id,
        content="memory that will decay",
        memory_type="episodic",
        importance=0.8
    )
    
    # Make it old and rarely accessed
    ltm.memories[decay_memory_id].encoding_time = datetime.now() - timedelta(days=100)
    ltm.memories[decay_memory_id].last_access = datetime.now() - timedelta(days=50)
    ltm.memories[decay_memory_id].access_count = 1
    ltm.memories[decay_memory_id].confidence = 0.9
    
    print(f"   - Before decay: importance={ltm.memories[decay_memory_id].importance:.3f}, confidence={ltm.memories[decay_memory_id].confidence:.3f}")
    
    # Apply decay
    decayed_count = ltm.decay_memories(decay_rate=0.1, half_life_days=30.0)
    print(f"   - Decayed {decayed_count} memories")
    print(f"   - After decay: importance={ltm.memories[decay_memory_id].importance:.3f}, confidence={ltm.memories[decay_memory_id].confidence:.3f}")
    
    # Test 3: Consolidation Tracking
    print("\n3. üîÑ Testing Consolidation Tracking")
    
    # Create STM items for consolidation
    stm_items = [
        MemoryItem(
            id=str(uuid.uuid4()),
            content="important learning experience",
            encoding_time=datetime.now() - timedelta(minutes=30),
            last_access=datetime.now() - timedelta(minutes=5),
            importance=0.8,
            access_count=5,
            emotional_valence=0.6  # Positive emotion
        ),
        MemoryItem(
            id=str(uuid.uuid4()),
            content="mundane daily activity",
            encoding_time=datetime.now() - timedelta(hours=25),
            last_access=datetime.now() - timedelta(hours=24),
            importance=0.2,
            access_count=1,
            emotional_valence=0.0
        )
    ]
    
    # Test consolidation
    consolidated_count = ltm.consolidate_from_stm(stm_items)
    print(f"   - Consolidated {consolidated_count} items from STM")
    
    # Check consolidation tracking
    recent_consolidated = ltm.get_recently_consolidated(hours=1)
    print(f"   - {len(recent_consolidated)} recently consolidated memories")
    
    stats = ltm.get_consolidation_stats()
    print(f"   - Consolidation stats: {stats['total_consolidated']} total, {stats['recent_24h']} in 24h")
    
    # Test 4: Meta-cognitive Feedback
    print("\n4. ü§î Testing Meta-cognitive Feedback")
    
    # Perform some searches to generate statistics
    ltm.search_by_content("programming", max_results=5)
    ltm.search_by_content("nonexistent topic", max_results=5)
    ltm.search_by_content("learning", max_results=5)
    
    # Get meta-cognitive stats
    meta_stats = ltm.get_metacognitive_stats()
    print(f"   - Search queries: {meta_stats['search_queries']}")
    print(f"   - Avg results per search: {meta_stats['avg_results_per_search']:.1f}")
    print(f"   - Total retrievals: {meta_stats['total_retrievals']}")
    print(f"   - Retrieval success rate: {meta_stats['retrieval_success_rate']:.2f}")
    
    # Get memory health report
    health_report = ltm.get_memory_health_report()
    print(f"   - Memory categories: {health_report['memory_categories']}")
    print(f"   - Potential issues: {health_report['potential_issues']}")
    print(f"   - Recommendations: {len(health_report['recommendations'])}")
    for rec in health_report['recommendations']:
        print(f"     ‚Ä¢ {rec}")
    
    # Test 5: Cross-system Query/Linking
    print("\n5. üîó Testing Cross-system Query/Linking")
    
    # Create some test memories for linking
    memory1_id = str(uuid.uuid4())
    memory2_id = str(uuid.uuid4())
    
    ltm.store(
        memory_id=memory1_id,
        content="Python programming best practices",
        memory_type="semantic",
        importance=0.7,
        tags=["programming", "python", "best_practices"]
    )
    
    ltm.store(
        memory_id=memory2_id,
        content="JavaScript async programming patterns",
        memory_type="semantic",
        importance=0.6,
        tags=["programming", "javascript", "async"]
    )
    
    # Test cross-system linking
    ltm.create_cross_system_link(memory1_id, memory2_id, "related_programming_concepts")
    
    # Find cross-system links
    links = ltm.find_cross_system_links(memory1_id)
    print(f"   - Found {len(links)} cross-system links for memory1")
    
    # Get semantic clusters
    clusters = ltm.get_semantic_clusters(min_cluster_size=2)
    print(f"   - Found {len(clusters)} semantic clusters")
    
    # Get cross-system suggestions (mock external memories)
    mock_external_memories = [
        {"id": "ext_1", "content": "Python best practices guide", "tags": ["python", "guide"]},
        {"id": "ext_2", "content": "JavaScript tutorials", "tags": ["javascript", "tutorial"]}
    ]
    suggestions = ltm.suggest_cross_system_associations(mock_external_memories, "external_system")
    print(f"   - Found {len(suggestions)} cross-system suggestions")
    
    # Test 6: Emotionally Weighted Consolidation
    print("\n6. üíù Testing Emotionally Weighted Consolidation")
    
    # Create STM items with different emotional valences
    emotional_stm_items = [
        MemoryItem(
            id=str(uuid.uuid4()),
            content="traumatic failure experience",
            encoding_time=datetime.now() - timedelta(minutes=15),
            last_access=datetime.now() - timedelta(minutes=1),
            importance=0.4,  # Low importance but high emotion
            access_count=2,
            emotional_valence=-0.9  # Strong negative emotion
        ),
        MemoryItem(
            id=str(uuid.uuid4()),
            content="amazing success celebration",
            encoding_time=datetime.now() - timedelta(minutes=20),
            last_access=datetime.now() - timedelta(minutes=2),
            importance=0.4,  # Low importance but high emotion
            access_count=2,
            emotional_valence=0.8  # Strong positive emotion
        ),
        MemoryItem(
            id=str(uuid.uuid4()),
            content="neutral routine task",
            encoding_time=datetime.now() - timedelta(hours=25),
            last_access=datetime.now() - timedelta(hours=24),
            importance=0.4,
            access_count=2,
            emotional_valence=0.0  # No emotion
        )
    ]
    
    # Test emotional consolidation
    emotional_consolidated = ltm.consolidate_from_stm(emotional_stm_items)
    print(f"   - Consolidated {emotional_consolidated} emotionally-influenced items")
    print("   - Emotional memories prioritized despite lower importance scores")
    
    # Final summary
    print("\nüéâ Enhanced LTM Features Test Summary")
    print("=" * 50)
    print(f"‚úÖ Total memories in LTM: {len(ltm.memories)}")
    print("‚úÖ Salience/Recency weighting: Working")
    print(f"‚úÖ Decay/Forgetting: {decayed_count} memories decayed")
    print(f"‚úÖ Consolidation tracking: {stats['total_consolidated']} tracked")
    print(f"‚úÖ Meta-cognitive feedback: {meta_stats['search_queries']} operations monitored")
    print(f"‚úÖ Cross-system linking: {len(links)} links created")
    print(f"‚úÖ Emotional consolidation: {emotional_consolidated} emotion-weighted items")
    print("\nüß† All enhanced LTM features are working correctly!")

if __name__ == "__main__":
    test_enhanced_ltm_features()
